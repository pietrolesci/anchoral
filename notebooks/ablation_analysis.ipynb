{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_18\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_8\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-importance-kmeans-True-50_2023-07-03T18-06-03_active_train_52\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_23\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-uniform-kmeans-True-50_2023-07-03T18-06-03_active_train_113\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_3\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_28\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_33\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_38\n",
      "../outputs/multirun/ablations/amazon/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_13\n",
      "../outputs/multirun/ablations/eurlex/anchoral-10000-uniform-random-True-50_2023-06-29T15-51-56_active_train_45\n",
      "../outputs/multirun/ablations/eurlex/anchoral-10000-uniform-random-True-50_2023-06-29T15-51-56_active_train_15\n",
      "../outputs/multirun/ablations/eurlex/anchoral-10000-uniform-random-True-50_2023-06-29T15-51-56_active_train_90\n",
      "../outputs/multirun/ablations/wiki_toxic/anchoral-10000-importance-kmeans-True-50_2023-07-01T22-02-05_active_train_3\n",
      "../outputs/multirun/ablations/wiki_toxic/anchoral-10000-importance-kmeans-True-50_2023-07-01T22-02-05_active_train_33\n",
      "../outputs/multirun/ablations/wiki_toxic/anchoral-10000-importance-kmeans-True-50_2023-07-01T22-02-05_active_train_23\n",
      "../outputs/multirun/ablations/wiki_toxic/anchoral-10000-importance-kmeans-True-50_2023-07-01T22-02-05_active_train_43\n",
      "../outputs/multirun/ablations/wiki_toxic/anchoral-10000-importance-kmeans-True-50_2023-07-01T22-02-05_active_train_73\n",
      "../outputs/multirun/ablations/wiki_toxic/anchoral-10000-importance-kmeans-True-50_2023-07-01T22-02-05_active_train_63\n",
      "../outputs/multirun/ablations/pubmed/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_12\n",
      "../outputs/multirun/ablations/pubmed/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_22\n",
      "../outputs/multirun/ablations/pubmed/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_2\n",
      "../outputs/multirun/ablations/pubmed/anchoral-25-topk-kmeans-True-50_2023-07-03T16-01-00_active_train_32\n"
     ]
    }
   ],
   "source": [
    "# CHECK MISSING\n",
    "for p in list(Path(\"../outputs/multirun/ablations/\").glob(\"*/*\")):\n",
    "    if not (p / \"tb_logs.parquet\").exists():\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14/tb_logs'),\n",
       " PosixPath('../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14/.early_stopping.jsonl'),\n",
       " PosixPath('../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14/.hydra'),\n",
       " PosixPath('../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14/logs'),\n",
       " PosixPath('../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14/active_train.log'),\n",
       " PosixPath('../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14/hparams.yaml')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\n",
    "    Path(\n",
    "        \"../outputs/multirun/ablations/amazon/anchoral-25-importance-random-True-50_2023-07-03T18-06-03_active_train_14\"\n",
    "    ).iterdir()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations, product\n",
    "from pathlib import Path\n",
    "\n",
    "import duckdb\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import polars as pl\n",
    "import pygwalker as pyg\n",
    "import seaborn as sns\n",
    "import srsly\n",
    "from matplotlib import ticker\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_fmt_str_lengths(100)\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"font.family\"] = \"monospace\"  # \"DejaVu Sans Mono\"\n",
    "plt.style.use(\"bmh\")\n",
    "sns.set_context(\"paper\")\n",
    "col_order = [\"amazon\", \"pubmed\", \"eurlex\", \"agnews\", \"wiki_toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = []\n",
    "\n",
    "for p in tqdm(list(Path(\"../outputs/multirun/\").rglob(\"ablations/*/*/tb_logs.parquet\"))):\n",
    "    meta = srsly.read_yaml(p.parent / \"hparams.yaml\")\n",
    "    hparams.append(\n",
    "        dict(\n",
    "            filename=str(p.parent),\n",
    "            data_seed=meta[\"data\"][\"seed\"],\n",
    "            model_seed=meta[\"model\"][\"seed\"],\n",
    "            initial_seed=meta[\"active_data\"][\"seed\"],\n",
    "            global_seed=meta[\"seed\"],\n",
    "            retriever=meta[\"index_metric\"],\n",
    "            dataset=meta[\"dataset\"][\"short_name\"],\n",
    "            **meta[\"strategy\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "hparams_df = pd.DataFrame(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hparams = hparams_df.nunique()\n",
    "changing_hparams = unique_hparams[unique_hparams > 1].index.tolist()\n",
    "hparams_df = hparams_df.loc[:, changing_hparams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_query = \"\"\"\n",
    "SELECT tag, count(tag)\n",
    "FROM read_parquet('../outputs/multirun/ablations/*/*/tb_logs.parquet', filename=True)\n",
    "GROUP BY tag\n",
    "\"\"\"\n",
    "metrics = con.execute(count_query).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.query(\"tag.str.contains('summary|search')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM read_parquet('../outputs/multirun/ablations/*/*/tb_logs.parquet', filename=True)\n",
    "WHERE tag IN (\n",
    "    'test/f1_class1',\n",
    "    'test/recall_class1',\n",
    "    'test/precision_class1',\n",
    "    'test/loss',\n",
    "    'summary/unique_ids_retrieved',\n",
    "    'summary/cumulative_count_class_1',\n",
    "    'summary/num_anchors',\n",
    "    'search/unique_ids_retrieved',\n",
    "    'summary/count_class_1',\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "all_df = con.execute(query).df()\n",
    "all_df = all_df.assign(\n",
    "    filename=lambda _df: _df[\"filename\"].map(lambda ex: str(Path(ex).parent)),\n",
    "    auc=lambda _df: _df.groupby([\"filename\", \"tag\"])[\"value\"].transform(np.trapz),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(all_df, hparams_df, on=\"filename\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = []\n",
    "\n",
    "for p in tqdm(list(Path(\"../outputs/multirun/\").rglob(\"test_new_kmeans/*/*/tb_logs.parquet\"))):\n",
    "    meta = srsly.read_yaml(p.parent / \"hparams.yaml\")\n",
    "    hparams.append(\n",
    "        dict(\n",
    "            filename=str(p.parent),\n",
    "            data_seed=meta[\"data\"][\"seed\"],\n",
    "            model_seed=meta[\"model\"][\"seed\"],\n",
    "            initial_seed=meta[\"active_data\"][\"seed\"],\n",
    "            global_seed=meta[\"seed\"],\n",
    "            retriever=meta[\"index_metric\"],\n",
    "            dataset=meta[\"dataset\"][\"short_name\"],\n",
    "            **meta[\"strategy\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "tmp_hparams_df = pd.DataFrame(hparams)\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM read_parquet('../outputs/multirun/test_new_kmeans/*/*/tb_logs.parquet', filename=True)\n",
    "WHERE tag IN (\n",
    "    'test/f1_class1',\n",
    "    'test/recall_class1',\n",
    "    'test/precision_class1',\n",
    "    'test/loss',\n",
    "    'summary/unique_ids_retrieved',\n",
    "    'summary/cumulative_count_class_1',\n",
    "    'summary/num_anchors',\n",
    "    'search/unique_ids_retrieved',\n",
    "    'summary/count_class_1',\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "tmp_all_df = con.execute(query).df()\n",
    "tmp_all_df = tmp_all_df.assign(\n",
    "    filename=lambda _df: _df[\"filename\"].map(lambda ex: str(Path(ex).parent)),\n",
    "    auc=lambda _df: _df.groupby([\"filename\", \"tag\"])[\"value\"].transform(np.trapz),\n",
    ")\n",
    "tmp_df = pd.merge(tmp_all_df, tmp_hparams_df, on=\"filename\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.loc[\n",
    "    (df[\"dataset\"].isin(tmp_df[\"dataset\"].unique()))\n",
    "    & (df[\"subpool_size\"].isin(tmp_df[\"subpool_size\"].unique()))\n",
    "    & (df[\"subpool_sampling_strategy\"].isin(tmp_df[\"subpool_sampling_strategy\"].unique()))\n",
    "]\n",
    "\n",
    "b = tmp_df.assign(anchor_strategy=lambda _df: _df[\"anchor_strategy\"] + \"_new\")\n",
    "\n",
    "c = pd.concat([a, b])\n",
    "collate_col = sorted(\n",
    "    [\n",
    "        i\n",
    "        for i in changing_hparams\n",
    "        if \"seed\" not in i and i not in (\"filename\", \"dataset\", \"anchor_strategy\")\n",
    "    ]\n",
    ")\n",
    "c[\"exp_id\"] = c[collate_col].astype(str).T.agg(\"-\".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK THAT THE NUMBER OF MINORITY COINCIDES UP TO THE INITIAL BUDGET\n",
    "\n",
    "# query = \"\"\"\n",
    "# SELECT filename, labelling_round as step, count(1) as num_minority\n",
    "# FROM read_parquet('../outputs/multirun/ablations/*/*/*/labelled_dataset.parquet', filename=True)\n",
    "# WHERE labels == 1\n",
    "# GROUP BY filename, step\n",
    "# \"\"\"\n",
    "\n",
    "# minority_count = con.execute(query).df()\n",
    "# minority_count = minority_count.assign(\n",
    "#     filename=lambda _df: _df[\"filename\"].map(lambda ex: str(Path(ex).parents[1]))\n",
    "# )\n",
    "\n",
    "# (\n",
    "#     pd.merge(\n",
    "#         all_df.query(\"tag == 'summary/count_class_1'\"),\n",
    "#         minority_count.query(\"step >= 0\").assign(step=lambda _df: _df[\"step\"] - 1),\n",
    "#         on=[\"filename\", \"step\"],\n",
    "#         how=\"inner\",\n",
    "#     )\n",
    "#     .assign(diff=lambda _df: _df[\"value\"] - _df[\"num_minority\"])\n",
    "#     [\"diff\"].sum()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_col = sorted(\n",
    "    [i for i in changing_hparams if \"seed\" not in i and i not in (\"filename\", \"dataset\")]\n",
    ")\n",
    "df[\"exp_id\"] = df[collate_col].astype(str).T.agg(\"-\".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = df.groupby([\"exp_id\", \"dataset\"])[\"filename\"].size().reset_index()\n",
    "check_df.loc[check_df[\"filename\"] < check_df[\"filename\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_metrics = df[\"tag\"].unique().tolist()\n",
    "loaded_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"anchor_strategy\"] == \"kmeans\"][collate_col].drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analaysis of AnchorAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"summary/num_anchors\",\n",
    "    \"search/unique_ids_retrieved\",\n",
    "    \"summary/count_class_1\",\n",
    "    \"test/f1_class1\",\n",
    "    # 'test/loss',\n",
    "    \"test/precision_class1\",\n",
    "    \"test/recall_class1\",\n",
    "]\n",
    "\n",
    "plot_data = df.loc[\n",
    "    (df[\"tag\"].isin(metrics))\n",
    "    & (df[\"only_minority\"] == True)\n",
    "    & (df[\"subpool_sampling_strategy\"] == \"importance\")\n",
    "    & (df[\"subpool_size\"] == 25)\n",
    "    & (df[\"num_neighbours\"] == 2000)\n",
    "].assign(step=lambda _df: (_df[\"step\"] * 25) + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_col = \"exp_id\"\n",
    "legend_order = sorted(plot_data[hue_col].unique().tolist())\n",
    "palette = dict(zip(legend_order, sns.color_palette(\"Set1\", n_colors=len(legend_order))))\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    plot_data,\n",
    "    row=\"tag\",\n",
    "    col=\"dataset\",\n",
    "    # col_wrap=3,\n",
    "    sharex=True,\n",
    "    legend_out=True,\n",
    "    despine=True,\n",
    "    # xlim=(0, xlim),\n",
    "    # ylim=ylim,\n",
    "    sharey=False,\n",
    "    # col_order=col_order,\n",
    "    margin_titles=True,\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    data=plot_data,\n",
    "    x=\"step\",\n",
    "    y=\"value\",\n",
    "    errorbar=(\"se\", 1),\n",
    "    hue=hue_col,\n",
    "    palette=palette,\n",
    "    style=\"tag\",\n",
    ")\n",
    "g.set_axis_labels(\"\", \"\")\n",
    "g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "g.add_legend(label_order=legend_order, bbox_to_anchor=(0.5, 1.02), title=hue_col)\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    (df[\"tag\"].isin(metrics))\n",
    "    & (df[\"subpool_size\"].isin([25]))\n",
    "    & (\n",
    "        df[\"num_neighbours\"].isin(\n",
    "            [\n",
    "                2000,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    & (df[\"only_minority\"] == True)\n",
    "].groupby([\"dataset\", \"anchor_strategy\"])[\"filename\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"test/f1_class1\",\n",
    "    \"summary/cumulative_count_class_1\",\n",
    "    \"summary/num_anchors\",\n",
    "    \"search/unique_ids_retrieved\",\n",
    "    # 'test/loss',\n",
    "    # 'test/precision_class1',\n",
    "    # 'test/recall_class1'\n",
    "]\n",
    "\n",
    "plot_data = df.loc[\n",
    "    (df[\"tag\"].isin(metrics))\n",
    "    & (df[\"subpool_size\"].isin([25]))\n",
    "    & (\n",
    "        df[\"num_neighbours\"].isin(\n",
    "            [\n",
    "                2000,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    & (df[\"only_minority\"] == True)\n",
    "].assign(step=lambda _df: (_df[\"step\"] * 25) + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = c.loc[\n",
    "    (c[\"tag\"].isin(metrics))\n",
    "    & (c[\"subpool_size\"].isin([25]))\n",
    "    & (\n",
    "        c[\"num_neighbours\"].isin(\n",
    "            [\n",
    "                2000,\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    & (c[\"only_minority\"] == True)\n",
    "    & (c[\"anchor_strategy\"].isin([\"kmeans\", \"kmeans_new\"]))\n",
    "].assign(step=lambda _df: (_df[\"step\"] * 25) + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data.groupby([\"exp_id\", \"dataset\"])[\"filename\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.relplot(\n",
    "    data=plot_data,\n",
    "    row=\"tag\",\n",
    "    col=\"dataset\",\n",
    "    x=\"step\",\n",
    "    y=\"value\",\n",
    "    hue=\"exp_id\",\n",
    "    style=\"anchor_strategy\",\n",
    "    kind=\"line\",\n",
    "    facet_kws=dict(sharex=True, sharey=False, margin_titles=True),\n",
    "    errorbar=(\"se\", 0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc(\"figure\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\n",
    "    (df[\"dataset\"] == \"amazon\")\n",
    "    & (df[\"anchor_strategy\"] == \"kmeans\")\n",
    "    & (df[\"subpool_sampling_strategy\"] == \"topk\")\n",
    "    & (df[\"only_minority\"] == True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_col = \"exp_id\"\n",
    "style_col = \"anchor_strategy\"\n",
    "\n",
    "# a = hue_col\n",
    "# hue_col = style_col\n",
    "# style_col = a\n",
    "\n",
    "legend_order = sorted(plot_data[hue_col].unique().tolist())\n",
    "palette = dict(zip(legend_order, sns.color_palette(\"bright\", n_colors=len(legend_order))))\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    plot_data,\n",
    "    row=\"tag\",\n",
    "    col=\"dataset\",\n",
    "    # col_wrap=3,\n",
    "    sharex=True,\n",
    "    legend_out=True,\n",
    "    despine=True,\n",
    "    # xlim=(0, xlim),\n",
    "    # ylim=ylim,\n",
    "    sharey=False,\n",
    "    # col_order=col_order,\n",
    "    margin_titles=True,\n",
    "    row_order=metrics,\n",
    ")\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    data=plot_data,\n",
    "    x=\"step\",\n",
    "    y=\"value\",\n",
    "    errorbar=(\"se\", 0),\n",
    "    hue=hue_col,\n",
    "    palette=palette,\n",
    "    style=style_col,\n",
    ")\n",
    "g.set_axis_labels(\"\", \"\")\n",
    "g.figure.subplots_adjust(hspace=0)\n",
    "g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "g.add_legend(label_order=legend_order, bbox_to_anchor=(0.5, 1.06), title=hue_col)\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    plot_data.query(\"tag == 'test/f1_class1'\")[[\"exp_id\", \"auc\", \"dataset\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby([\"exp_id\", \"dataset\"])[\"auc\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    "    .sort_values([\"dataset\", \"mean\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.query(\"(tag == 'test/f1_class1') & (subpool_size == 25)\")\n",
    "    .groupby([\"dataset\", \"exp_id\", \"filename\"])[\"value\"]\n",
    "    .apply(np.trapz)\n",
    "    .reset_index()\n",
    "    .groupby([\"dataset\", \"exp_id\"])[\"value\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .sort_values([\"dataset\", \"mean\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib as hb\n",
    "from datasets import load_from_disk\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\n",
    "    \"/home/pl487/anchoral/outputs/multirun/ablations/eurlex/anchoral-25-importance-kmeans-True-50_2023-07-03T18-06-03_active_train_140/logs/labelled_dataset.parquet\"\n",
    ")\n",
    "ds = load_from_disk(\"/home/pl487/anchoral/data/prepared/eurlex-57k_bert-tiny/\")[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = data.query(\"(labelling_round < 20) & (labels == 1)\")[\"uid\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = hb.Index(space=\"cosine\", dim=364)\n",
    "index.load_index(\"/home/pl487/anchoral/data/processed/eurlex-57k/all-MiniLM-L12-v2_cosine.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uids = index.get_ids_list()\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = np.stack(index.get_items(uids))\n",
    "X = normalize(X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_avg_n_clusters = []\n",
    "for n_clusters in range(2, len(uids)):\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    silhouette_avg_n_clusters.append(silhouette_avg)\n",
    "\n",
    "np.argmax(silhouette_avg_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(silhouette_avg_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(silhouette_avg_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "\n",
    "X = [[4, 1, 2, 2], [1, 3, 9, 3], [5, 7, 5, 1]]\n",
    "\n",
    "Normalizer().fit_transform(X), StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2)\n",
    "scaled_emb = StandardScaler().fit_transform(emb)\n",
    "scaled_emb = reducer.fit_transform(scaled_emb, ds[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HDBSCAN(min_cluster_size=500)\n",
    "clusterer.fit(scaled_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(clusterer.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=scaled_emb[:, 0], y=scaled_emb[:, 1], color=ds[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(x=scaled_emb[:, 0], y=scaled_emb[:, 1], z=scaled_emb[:, 2], color=ds[\"labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

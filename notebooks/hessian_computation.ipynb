{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from lightning.fabric import seed_everything\n",
    "from torch.types import Device\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.func import functional_call, grad, vmap  # type: ignore\n",
    "\n",
    "from energizer.datastores import PandasDataStoreForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher(\n",
    "    shape,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    device: Optional[Device] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Sample from Rademacher distribution.\"\"\"\n",
    "    return torch.randint(0, 2, shape, generator=generator, device=device) * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7e91dce23244beb4d5518017f634e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-999f0e823471e99e_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7c0ef1ff89bb0eab_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-db9896ae5bc093c4_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ds_dict = load_dataset(\"pietrolesci/pubmed-rct20k_indexed\").map(lambda ex: tokenizer(ex[\"text\"]), batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PandasDataStoreForSequenceClassification()\n",
    "ds.from_dataset_dict(\n",
    "    ds_dict,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    target_name=\"labels\",\n",
    "    tokenizer=tokenizer,\n",
    "    uid_name=\"uid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading(batch_size=1, eval_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2817, 16578, 11290,  3853, 28828,  1999,  2540,  4945,\n",
       "           5022,  4914,  2007,  5729, 11325, 21933,  8737,  6132,  4383,  2540,\n",
       "           4945,  1006,  4748,  2232,  2546,  1007,  1012,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1]]),\n",
       " <InputKeys.TARGET: 'labels'>: tensor([0])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = ds.show_batch(\"test\")\n",
    "_ = batch.pop(\"on_cpu\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "# load model using data properties\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,  # type: ignore\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "params = {k: v.detach() for k, v in model.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in model.named_buffers()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "    params: Dict, buffers: Dict, input_ids: Tensor, attention_mask: Tensor, labels: Tensor\n",
    ") -> torch.Tensor:\n",
    "    inp, att, lab = input_ids.unsqueeze(0), attention_mask.unsqueeze(0), labels.unsqueeze(0)\n",
    "    return functional_call(model, (params, buffers), (inp, att), kwargs={\"labels\": lab}).loss\n",
    "\n",
    "def _grad_norm(grads: Dict, norm_type: int) -> Tensor:\n",
    "    norms = [g.norm(norm_type).unsqueeze(0) for g in grads.values() if g is not None]\n",
    "    return torch.concat(norms).norm(norm_type)\n",
    "\n",
    "grad_norm = partial(_grad_norm, norm_type=2)\n",
    "compute_grad = grad(compute_loss)\n",
    "\n",
    "def compute_grad_norm(\n",
    "    params: Dict, buffers: Dict, input_ids: Tensor, attention_mask: Tensor, labels: Tensor\n",
    ") -> torch.Tensor:\n",
    "    grads = compute_grad(params, buffers, input_ids, attention_mask, labels)\n",
    "    return grad_norm(grads)\n",
    "\n",
    "compute_grad_norm_vect = vmap(compute_grad_norm, in_dims=(None, None, 0, 0, 0), randomness=\"same\")\n",
    "\n",
    "norms, ids = [], []\n",
    "for batch in tqdm(datastore.train_loader(), disable=True):\n",
    "    batch = self.transfer_to_device(batch)\n",
    "    b_inp, b_att, b_lab = batch[InputKeys.INPUT_IDS], batch[InputKeys.ATT_MASK], batch[InputKeys.TARGET]\n",
    "    norms += compute_grad_norm_vect(params, buffers, b_inp, b_att, b_lab).tolist()\n",
    "    ids += batch[InputKeys.ON_CPU][SpecialKeys.ID]\n",
    "\n",
    "norms = np.array(norms)\n",
    "ids = np.array(ids)\n",
    "topk_ids = norms.argsort()[-self.num_influential :]  # biggest gradient norm\n",
    "return ids[topk_ids].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "loss = model(**batch).loss\n",
    "torch.autograd.backward(loss, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "generator = torch.Generator().manual_seed(2147483647)\n",
    "grads = [p.grad for p in params]\n",
    "n_samples = 10\n",
    "\n",
    "\n",
    "for i in range(n_samples):\n",
    "    zs = [\n",
    "        torch.randint(0, 2, p.size(), generator=generator) * 2.0 - 1.0\n",
    "        for p in params\n",
    "    ]  # Rademacher distribution {-1.0, 1.0}\n",
    "\n",
    "    h_zs = torch.autograd.grad(grads, params, grad_outputs=zs, retain_graph=False)\n",
    "\n",
    "    if i == 0:\n",
    "        hess = h_zs\n",
    "    else:\n",
    "        for d, hvp, v in zip(hess, h_zs, zs):\n",
    "            d += v * hvp / n_samples\n",
    "\n",
    "    # for idx, (h_z, z) in enumerate(zip(h_zs, zs)):\n",
    "    #     hess[idx] += h_z / n_samples  # approximate the expected values of z*(H@z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "generator = torch.Generator().manual_seed(2147483647)\n",
    "grads = [p.grad for p in params]\n",
    "n_samples = 10\n",
    "\n",
    "for _ in range(V):\n",
    "    for p in model.parameters():\n",
    "        v = [rademacher(p.size())]\n",
    "        Hv = hessian_vector_product(loss, [p], v)\n",
    "        vHv = torch.einsum(\"i,i->\", v[0].flatten(), Hv[0].flatten())\n",
    "\n",
    "        trace += vHv / V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rademacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess = []\n",
    "hess[1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grads), len(params), len(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs[0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

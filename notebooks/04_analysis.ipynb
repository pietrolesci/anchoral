{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import duckdb as db\n",
    "import pandas as pd\n",
    "import plotnine as pn\n",
    "from datasets import load_from_disk\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read hparams for all experiments and the metrics, thus creating a unique dataset to perform the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(s: str) -> str:\n",
    "    _s = s.split(\"/\")\n",
    "    g = \"-\".join(_s[2:4]) if \"ablations\" in s or \"additional\" in s else _s[2]\n",
    "    return g.strip()\n",
    "\n",
    "\n",
    "def fix_name(s: str) -> str:\n",
    "    if \"_\" in s:\n",
    "        return s\n",
    "    return f\"noop_{s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam_df = pd.read_csv(path / \"hparams.tsv\", sep=\"\\t\")\n",
    "\n",
    "hparam_df[\"experiment_group\"] = hparam_df[\"filename\"].map(get_group)\n",
    "hparam_df[\"model.name\"] = hparam_df[\"model.name\"].replace(\"bert-base\", \"bert-base-uncased\")\n",
    "hparam_df[\"strategy.name\"] = hparam_df[\"strategy.name\"].map(fix_name)\n",
    "\n",
    "# remove columns with one unique value\n",
    "hparam_df = hparam_df.iloc[:, (hparam_df.nunique() > 1).values]  # type: ignore\n",
    "\n",
    "# remove specific columns\n",
    "hparam_df = hparam_df.iloc[\n",
    "    :,\n",
    "    ~hparam_df.columns.str.contains(\n",
    "        \"loggers|data_path|learning_rate|run_name|callbacks|seed|prepared_path|processed_path\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# remove columns with all NA\n",
    "hparam_df = hparam_df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# cols = hparam_df.columns[hparam_df.columns.str.contains(\"active_fit.*|strategy.args*\")].tolist() +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = pd.read_parquet(path / \"metrics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(metric_df, hparam_df, on=\"filename\", how=\"left\")\n",
    "\n",
    "# aggregate classes into minority and majority\n",
    "df[\"variable\"] = \"f1_majority\"\n",
    "df.loc[df[\"tag\"].str.contains(\"timer\"), \"variable\"] = \"time\"\n",
    "df.loc[\n",
    "    ((df[\"dataset.name\"] == \"amazon-multi\") & (df[\"tag\"] != \"test/f1_class4\") & (~df[\"tag\"].str.contains(\"timer\")))\n",
    "    | ((df[\"dataset.name\"] != \"amazon-multi\") & (df[\"tag\"] == \"test/f1_class1\")),\n",
    "    \"variable\",\n",
    "] = \"f1_minority\"\n",
    "df = df.drop(columns=[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_value(m: float, iqr: float) -> str:\n",
    "    return rf\"$\\float[1]{{{round(m, 2)}}}_\" + r\"{\\pm\" + rf\"\\float[1]{{{round(iqr, 2)}}}\" + \"}$\"\n",
    "\n",
    "\n",
    "def format_int(m: int) -> str:\n",
    "    _m = int(m)\n",
    "    return rf\"\\integer{{{int(_m)}}}\" if _m < 1000 else rf\"\\q{{{round(_m / 1000, ndigits=1)}}}\" + \"{\\thousand}\"\n",
    "\n",
    "\n",
    "def format_float(m: float) -> str:\n",
    "    return rf\"\\float[1]{{{m}}}\"\n",
    "\n",
    "\n",
    "variable_name = {\"f1_majority\": r\"\\textbf{Majority}\", \"f1_minority\": r\"\\textbf{Minority}\", \"time\": r\"\\textbf{Time}\"}\n",
    "\n",
    "dataset_name = {\n",
    "    \"f1_majority\": \"Majority\",\n",
    "    \"f1_minority\": \"Minority\",\n",
    "    \"agnews-business-.01\": r\"\\agnewsbus\",\n",
    "    \"amazon-agri\": r\"\\amazonagri\",\n",
    "    \"amazon-multi\": r\"\\amazonmulti\",\n",
    "    \"wikitoxic-.01\": r\"\\wikitoxic\",\n",
    "}\n",
    "\n",
    "model_name = {\n",
    "    \"bert-base-uncased\": r\"\\bertbase\",\n",
    "    \"albert-base-v2\": r\"\\albertbase\",\n",
    "    \"bert-tiny\": r\"\\berttiny\",\n",
    "    \"deberta_v3-base\": r\"\\debertabase\",\n",
    "    \"gpt2\": r\"\\gpt\",\n",
    "    \"t5-base\": r\"\\tf\",\n",
    "}\n",
    "\n",
    "anchorstrategy_name = {\"kmeans_pp_sampling\": r\"\\myemph{KM++}\", \"entropy\": r\"\\myemph{Ent}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute ablations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = hparam_df.columns[hparam_df.columns.str.contains(\"active_fit.*|strategy.args*\")].tolist()\n",
    "cols += [\"experiment_group\", \"dataset.name\", \"model.name\", \"strategy.name\", \"variable\", \"step\"]\n",
    "\n",
    "agg_df = (\n",
    "    df.query(\n",
    "        \"((experiment_group.str.startswith('ablation')) & (~experiment_group.str.contains('super')))\"\n",
    "        \"| ((experiment_group == 'main') & (`model.name` == 'bert-base-uncased')\"\n",
    "        \"& (`strategy.name` == 'anchoral_entropy')\"\n",
    "        \"& (`dataset.name` == 'amazon-agri'))\"\n",
    "        # \"| (experiment_group.str.contains('1000'))\"\n",
    "    )\n",
    "    .fillna(100000)\n",
    "    # compute median and IQR\n",
    "    .groupby(cols)[\"value\"]\n",
    "    .quantile([0.25, 0.5, 0.75])  # type: ignore\n",
    "    .unstack(-1)\n",
    "    .assign(iqr=lambda _df: _df[0.75] - _df[0.25])\n",
    "    .rename(columns={0.5: \"median\"})\n",
    "    .drop(columns=[0.25, 0.75])\n",
    "    .reset_index()\n",
    "    # filter for last step\n",
    "    .assign(max_step=lambda _df: _df.groupby(cols[:-2])[\"step\"].max().min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df[\"experiment_group\"].unique().tolist(), agg_df[\"max_step\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    r\"$\\anchorstrategy_{\\mathtt{maj}}$\",\n",
    "    r\"$\\anchorstrategy_{\\mathtt{min}}$\",\n",
    "    r\"$\\numanchors$\",\n",
    "    r\"$\\numneighbours$\",\n",
    "    \"variable\",\n",
    "    \"value\",\n",
    "]\n",
    "\n",
    "abl_tbl = (\n",
    "    agg_df.query(\"step == max_step\")\n",
    "    .drop(columns=\"max_step\")\n",
    "    .assign(\n",
    "        **{\n",
    "            r\"$\\anchorstrategy_{\\mathtt{maj}}$\": lambda _df: _df[\"strategy.args.anchor_strategy_majority\"].map(\n",
    "                anchorstrategy_name\n",
    "            ),\n",
    "            r\"$\\anchorstrategy_{\\mathtt{min}}$\": lambda _df: _df[\"strategy.args.anchor_strategy_minority\"].map(\n",
    "                anchorstrategy_name\n",
    "            ),\n",
    "            r\"$\\numneighbours$\": lambda _df: _df[\"strategy.args.num_neighbours\"].map(format_int),\n",
    "            r\"$\\numanchors$\": lambda _df: _df[\"strategy.args.num_anchors\"].map(format_int),\n",
    "            # \"value\": lambda _df: _df.apply(lambda row: format_value(row[\"median\"], row[\"iqr\"]), axis=1),\n",
    "            \"value\": lambda _df: _df[\"median\"].map(format_float),\n",
    "            \"variable\": lambda _df: _df[\"variable\"].map(variable_name),\n",
    "        }\n",
    "    )\n",
    "    .loc[:, cols]\n",
    "    .set_index(cols[:-1])\n",
    "    .unstack(\"variable\")\n",
    "    .droplevel(0, axis=1)  # type: ignore\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abl_tbl.to_latex(\"../results/ablation_table.tex\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute main results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"experiment_group\", \"dataset.name\", \"model.name\", \"strategy.name\", \"step\"]\n",
    "\n",
    "# consider only those steps with at least 3 evaluations\n",
    "dd = (\n",
    "    df.query(\"experiment_group.str.contains('main|other_models')\")\n",
    "    # .query(\"(~`strategy.name`.str.contains('noop')) | (`strategy.name`.str.contains('random'))\")\n",
    "    .groupby(cols)[\"filename\"]\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .query(\"(filename >= 3) | ((`strategy.name`.str.contains('noop')) & (~`strategy.name`.str.contains('random')))\")\n",
    "    .loc[:, cols]\n",
    ")\n",
    "\n",
    "# within all the steps with at least 3 evaluations, consider the maximum per experiment and per\n",
    "# dataset-model combination\n",
    "dd = dd.assign(max_step_exp=lambda _df: _df.groupby(cols[:-1])[\"step\"].transform(\"max\")).assign(\n",
    "    min_max_step=lambda _df: _df.query(\n",
    "        \"(~`strategy.name`.str.contains('noop')) | (`strategy.name`.str.contains('random'))\"\n",
    "    )\n",
    "    .groupby(cols[:-2])[\"max_step_exp\"]\n",
    "    .transform(\"min\")\n",
    ")\n",
    "dd[\"min_max_step\"] = dd[\"min_max_step\"].fillna(dd[\"max_step_exp\"]).astype(int)\n",
    "dd[\"strategy.name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"experiment_group\", \"dataset.name\", \"model.name\", \"strategy.name\", \"variable\", \"step\"]\n",
    "\n",
    "agg_df = (\n",
    "    df.query(\"experiment_group.str.contains('main|other_models')\")\n",
    "    # compute median and IQR\n",
    "    .groupby(cols)[\"value\"]\n",
    "    .quantile([0.25, 0.5, 0.75])  # type: ignore\n",
    "    .unstack(-1)\n",
    "    .assign(iqr=lambda _df: _df[0.75] - _df[0.25])\n",
    "    .rename(columns={0.5: \"median\"})\n",
    "    .drop(columns=[0.25, 0.75])\n",
    "    .reset_index()\n",
    ")\n",
    "agg_df.loc[(agg_df[\"variable\"] == \"time\") & (agg_df[\"step\"] >= 195), \"step\"] = 196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"experiment_group\", \"dataset.name\", \"model.name\", \"strategy.name\", \"step\"]\n",
    "\n",
    "overall_df = pd.merge(dd.query(\"step == max_step_exp\"), agg_df, on=cols, how=\"left\")[\n",
    "    cols + [\"variable\", \"median\", \"iqr\"]\n",
    "]\n",
    "\n",
    "budget_df = pd.merge(dd.query(\"step == min_max_step\"), agg_df, on=cols, how=\"left\")[\n",
    "    cols + [\"variable\", \"median\", \"iqr\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_df.groupby(cols)[\"variable\"].nunique().reset_index().query(\"variable < 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# budget_df.groupby(cols)[\"variable\"].nunique().reset_index().query(\"variable < 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"(`dataset.name` == 'amazon-multi') & (`model.name` == 'bert-base-uncased')\")[\"variable\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"experiment_group\", \"model.name\", \"dataset.name\", \"strategy.name\", \"step\", \"variable\"]\n",
    "\n",
    "pdata = (\n",
    "    agg_df.set_index(cols)\n",
    "    .unstack(\"variable\")\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        strategy=lambda _df: _df[\"strategy.name\"].str.split(\"_\", expand=True)[1],\n",
    "        pool_filtering=lambda _df: _df[\"strategy.name\"].str.split(\"_\", expand=True)[0],\n",
    "    )\n",
    "    .assign(\n",
    "        pool_filtering=lambda _df: _df[\"pool_filtering\"].map(\n",
    "            {\"anchoral\": \"AnchorAL\", \"seals\": \"SEALS\", \"randomsubset\": \"RandSub\"}\n",
    "        ),\n",
    "        dataset=lambda _df: _df[\"dataset.name\"].map(\n",
    "            {\n",
    "                \"agnews-business-.01\": \"Agnews-Bus\",\n",
    "                \"amazon-agri\": \"Amazon-Agri\",\n",
    "                \"amazon-multi\": \"Amazon-Multi\",\n",
    "                \"wikitoxic-.01\": \"WikiToxic\",\n",
    "            }\n",
    "        ),\n",
    "        model=lambda _df: _df[\"model.name\"].map(\n",
    "            {\n",
    "                \"bert-base-uncased\": \"BERT-base\",\n",
    "                \"albert-base-v2\": \"ALBERT-base\",\n",
    "                \"bert-tiny\": \"BERT-tiny\",\n",
    "                \"deberta_v3-base\": \"DeBERTa-base\",\n",
    "                \"gpt2\": \"GPT-2\",\n",
    "                \"t5-base\": \"T5-base\",\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "pdata.columns = [\"_\".join(i).removesuffix(\"_\") for i in pdata.columns]\n",
    "pdata = pdata.dropna(subset=[\"pool_filtering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    r\"\\textbf{Dataset}\",\n",
    "    r\"\\textbf{Model}\",\n",
    "    r\"\\textbf{\\AL Strategy}\",\n",
    "    r\"\\textbf{Pool Filtering}\",\n",
    "    r\"\\textbf{Budget}\",\n",
    "    \"variable\",\n",
    "    \"value\",\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i, d in [(r\"\\textbf{Overall}\", overall_df), (r\"\\textbf{Budget-Matched}\", budget_df)]:\n",
    "    new_d = (\n",
    "        d.assign(\n",
    "            **{\n",
    "                \"value\": lambda _df: _df.apply(lambda row: format_value(row[\"median\"], row[\"iqr\"]), axis=1),\n",
    "                \"variable\": lambda _df: _df[\"variable\"].map(variable_name),\n",
    "                r\"\\textbf{Dataset}\": lambda _df: _df[\"dataset.name\"].map(dataset_name),\n",
    "                r\"\\textbf{Model}\": lambda _df: _df[\"model.name\"].map(model_name),\n",
    "                r\"\\textbf{\\AL Strategy}\": lambda _df: \"\\\\\" + _df[\"strategy.name\"].str.split(\"_\", expand=True)[1],\n",
    "                r\"\\textbf{Pool Filtering}\": lambda _df: \"\\\\\" + _df[\"strategy.name\"].str.split(\"_\", expand=True)[0],\n",
    "                r\"\\textbf{Budget}\": lambda _df: ((_df[\"step\"] * 25) + 100).map(format_int),\n",
    "            }\n",
    "        )\n",
    "        .loc[:, cols]\n",
    "        .set_index(cols[:-1])\n",
    "        .unstack(\"variable\")\n",
    "        .droplevel(0, axis=1)  # type: ignore\n",
    "        .reset_index()\n",
    "        .set_index(cols[:4])\n",
    "    )\n",
    "    new_d.columns = pd.MultiIndex.from_product([[i], new_d.columns.tolist()])\n",
    "\n",
    "    dfs.append(new_d)\n",
    "tbl = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super ugly, yet I need it\n",
    "tbl[(r\"\\textbf{Budget-Matched}\", r\"\\textbf{Budget}\")] = tbl.apply(\n",
    "    lambda row: row[(r\"\\textbf{Budget-Matched}\", r\"\\textbf{Budget}\")]\n",
    "    if row[(r\"\\textbf{Overall}\", r\"\\textbf{Budget}\")] == row[(r\"\\textbf{Budget-Matched}\", r\"\\textbf{Budget}\")]\n",
    "    else row[(r\"\\textbf{Budget-Matched}\", r\"\\textbf{Budget}\")] + r\" \\cellcolor{gray!30}\",\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbl.loc[tbl[(r'\\textbf{Overall}', r'\\textbf{Budget}')] != tbl[(r'\\textbf{Budget-Matched}', r'\\textbf{Budget}')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.fillna(\"-\").to_latex(\"../results/main_table.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_small = tbl.reset_index()\n",
    "tbl_small = tbl_small.loc[\n",
    "    (tbl_small[(r\"\\textbf{\\AL Strategy}\", \"\")] == r\"\\entropy\")\n",
    "    & (tbl_small[(r\"\\textbf{Model}\", \"\")] == r\"\\bertbase\")\n",
    "    & (tbl_small[(r\"\\textbf{Dataset}\", \"\")] != r\"\\agnewsbus\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    tbl_small.loc[:, tbl_small.columns[[0, 3, 4, 5, 6, 7, 9, 10]]]  # type: ignore\n",
    "    .set_index([r\"\\textbf{Dataset}\"])\n",
    "    .to_latex(\"../results/small_table.tex\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute labelling ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path / \"labelled_ids.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, hparam_df, on=\"filename\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"experiment_group\", \"model.name\", \"dataset.name\", \"strategy.name\", \"labelling_round\", \"labels\"]\n",
    "\n",
    "dd = (\n",
    "    df.assign(total=lambda _df: _df.groupby([\"filename\", \"labelling_round\"])[\"cum_n\"].transform(\"sum\"))\n",
    "    .query(\"experiment_group.str.contains('main|other_models')\")\n",
    "    .assign(n_exp=lambda _df: _df.groupby(cols)[\"filename\"].transform(\"nunique\"))\n",
    "    .query(\"(n_exp >= 3)\")\n",
    "    .loc[:, cols + [\"n\", \"cum_n\", \"total\", \"filename\"]]\n",
    ")\n",
    "dd[\"label\"] = \"minority\"\n",
    "dd.loc[\n",
    "    ((dd[\"dataset.name\"] == \"amazon-multi\") & (dd[\"labels\"] == 4))\n",
    "    | ((dd[\"dataset.name\"] != \"amazon-multi\") & (dd[\"labels\"] == 0)),\n",
    "    \"label\",\n",
    "] = \"majority\"\n",
    "\n",
    "\n",
    "dd = dd.assign(\n",
    "    cum_n=lambda _df: _df.groupby([\"filename\", \"labelling_round\", \"label\"])[\"cum_n\"].transform(\"sum\")\n",
    ").assign(\n",
    "    pool_filtering=lambda _df: _df[\"strategy.name\"].str.split(\"_\", expand=True)[0].str.strip(),\n",
    "    strategy=lambda _df: _df[\"strategy.name\"].str.split(\"_\", expand=True)[1].str.strip(),\n",
    "    p=lambda _df: _df[\"cum_n\"] / _df[\"total\"],\n",
    ")\n",
    "\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd[\"dataset.name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"dataset.name\", \"pool_filtering\", \"labelling_round\", \"label\"]\n",
    "\n",
    "pdata = (\n",
    "    dd.query(\"(label == 'minority') & (pool_filtering != 'noop') & (experiment_group == 'main')\")\n",
    "    .groupby(cols)[\"p\"]\n",
    "    .agg(gmean)\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        pool_filtering=lambda _df: _df[\"pool_filtering\"].map(\n",
    "            {\"anchoral\": \"AnchorAL\", \"seals\": \"SEALS\", \"randomsubset\": \"RandSub\"}\n",
    "        ),\n",
    "        dataset=lambda _df: _df[\"dataset.name\"].map(\n",
    "            {\n",
    "                \"agnews-business-.01\": \"Agnews-Bus\",\n",
    "                \"amazon-agri\": \"Amazon-Agri\",\n",
    "                \"amazon-multi\": \"Amazon-Multi\",\n",
    "                \"wikitoxic-.01\": \"WikiToxic\",\n",
    "            }\n",
    "        ),\n",
    "        labelling_round=lambda _df: (_df[\"labelling_round\"] * 25) + 100,\n",
    "    )\n",
    "    .assign(max_step=lambda _df: _df.groupby([\"pool_filtering\", \"dataset\"])[\"labelling_round\"].transform(\"max\"))\n",
    "    .assign(min_max_step=lambda _df: _df.groupby([\"dataset\"])[\"max_step\"].transform(\"min\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\n",
    "    pn.ggplot(pdata.query(\"labelling_round <= 1000\"), pn.aes(\"labelling_round\", \"p\", colour=\"pool_filtering\"))\n",
    "    + pn.geom_line()\n",
    "    + pn.geom_point()\n",
    "    + pn.scale_y_continuous(breaks=[0.05, 0.1, 0.2, 0.3], labels=lambda l: [\".\" + str(x).split(\".\")[1] for x in l])\n",
    "    +\n",
    "    pn.coord_cartesian(xlim=[100, 1000])  # type: ignore\n",
    "    + pn.facet_grid(\"dataset~.\", scales=\"free_y\")\n",
    "    + pn.theme_bw(base_size=12)\n",
    "    + pn.theme(legend_position=\"top\", legend_box_spacing=0.01, legend_box_margin=0)\n",
    "    + pn.labs(x=\"\", y=\"\", colour=\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.save(\"../results/minority_proportions.png\", format=\"png\", dpi=300, width=4, height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subpool analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = str(path / \"subpool_ids.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/prepared\")\n",
    "datasets = {\n",
    "    \"amazon-agri\": \"amazoncat-agri_bert-base-uncased\",\n",
    "    \"amazon-multi\": \"amazoncat-multi_bert-base-uncased\",\n",
    "    \"wikitoxic-.01\": \"wikitoxic-.01_bert-base-uncased\",\n",
    "    \"agnews-business-.01\": \"agnews-business-.01_bert-base-uncased\",\n",
    "}\n",
    "data = []\n",
    "for n, d in datasets.items():\n",
    "    a = (\n",
    "        load_from_disk(str(data_path / d))[\"train\"]\n",
    "        .select_columns([\"uid\", \"labels\"])    # type: ignore\n",
    "        .to_pandas()\n",
    "        .assign(name=n)    # type: ignore\n",
    "    )\n",
    "    data.append(a)\n",
    "\n",
    "labels = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = db.sql(\n",
    "    f\"\"\"\n",
    "with ctx as (\n",
    "    select *\n",
    "    from read_parquet('{p}') l left join hparam_df r on r.filename = l.filename\n",
    "    where experiment_group == 'main'\n",
    ")\n",
    "select \n",
    "    *, \n",
    "    split_part(\"strategy.name\", '_', 1) as pool_filtering,\n",
    "    case when\n",
    "        (name = 'amazon-multi' and labels = 4) or (name != 'amazon-multi' and labels = 0) then 'majority'\n",
    "        else 'minority'\n",
    "    end as label\n",
    "from ctx l left join labels r on l.\"dataset.name\" = r.name and l.subpool_ids == r.uid\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tbl = db.sql(\n",
    "    \"\"\"\n",
    "select \n",
    "    filename, labelling_round, label, count(1) as n\n",
    "from tbl\n",
    "group by filename, labelling_round, label\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = count_tbl.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, hparam_df, on=\"filename\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = (\n",
    "    df.assign(\n",
    "        total=lambda _df: _df.groupby([\"filename\", \"labelling_round\"])[\"n\"].transform(\"sum\"),\n",
    "        pool_filtering=lambda _df: _df[\"strategy.name\"].str.split(\"_\", expand=True)[0].str.strip(),\n",
    "        strategy=lambda _df: _df[\"strategy.name\"].str.split(\"_\", expand=True)[1].str.strip(),\n",
    "    )\n",
    "    .assign(p=lambda _df: _df[\"n\"] / _df[\"total\"])\n",
    "    .sort_values([\"filename\", \"labelling_round\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata = (\n",
    "    dd.query(\"label == 'minority'\")\n",
    "    .groupby([\"pool_filtering\", \"dataset.name\", \"labelling_round\"])[\"p\"]\n",
    "    .agg(gmean)\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        pool_filtering=lambda _df: _df[\"pool_filtering\"].map(\n",
    "            {\"anchoral\": \"AnchorAL\", \"seals\": \"SEALS\", \"randomsubset\": \"RandSub\"}\n",
    "        ),\n",
    "        dataset=lambda _df: _df[\"dataset.name\"].map(\n",
    "            {\n",
    "                \"agnews-business-.01\": \"Agnews-Bus\",\n",
    "                \"amazon-agri\": \"Amazon-Agri\",\n",
    "                \"amazon-multi\": \"Amazon-Multi\",\n",
    "                \"wikitoxic-.01\": \"WikiToxic\",\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\n",
    "    pn.ggplot(pdata.query(\"labelling_round <= 120\"), pn.aes(\"labelling_round\", \"p\", colour=\"pool_filtering\"))\n",
    "    + pn.facet_grid(\"dataset~.\", scales=\"free_y\")\n",
    "    + pn.geom_line()\n",
    "    + pn.geom_point()\n",
    "    + pn.scale_y_continuous(breaks=[0.05, 0.1, 0.2, 0.3], labels=lambda l: [\".\" + str(x).split(\".\")[1] for x in l])\n",
    "    +\n",
    "    # pn.scale_x_continuous(breaks=[.05, .1, .2, .3]) +\n",
    "    # pn.coord_cartesian(xlim=[100, 1000]) +  # type: ignore\n",
    "    pn.facet_grid(\"dataset~.\", scales=\"free_y\")\n",
    "    + pn.theme_bw(base_size=12)\n",
    "    + pn.theme(legend_position=\"top\", legend_box_spacing=0.01, legend_box_margin=0)\n",
    "    + pn.labs(x=\"\", y=\"\", colour=\"\")\n",
    ")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.save(\"../results/subpool_minority_proportions.png\", format=\"png\", dpi=300, width=4, height=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

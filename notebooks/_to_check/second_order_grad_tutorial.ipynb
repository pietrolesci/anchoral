{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import combinations, product\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import functorch as ft\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.functional import hessian, jacobian\n",
    "from torch.nn.utils.stateless import functional_call\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute second-order derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input space is the following\n",
    "$$x \\in \\mathbb{R}^2$$\n",
    "\n",
    "The output space is\n",
    "$$y \\in \\mathbb{R}$$\n",
    "\n",
    "The function $f$ is a scalar function\n",
    "$$f: \\mathbb{R}^2 \\to \\mathbb{R}$$\n",
    "\n",
    "defined as\n",
    "$$f(x) \\coloneqq Wx + b$$\n",
    "\n",
    "and the loss function is the MSE\n",
    "$$l(x, y; W) \\coloneqq (f_W(x) - y)^2$$\n",
    "\n",
    "For convenience define\n",
    "$$D = (x, y)$$\n",
    "\n",
    "$$\\theta = (W, b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand((2,), requires_grad=True)\n",
    "targets = torch.rand((1,), requires_grad=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    # torch.nn.Linear(2, 2),\n",
    "    torch.nn.Linear(2, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model functional manually\n",
    "params_dict = {k: v.detach().requires_grad_() for k, v in model.named_parameters()}\n",
    "names = deepcopy(tuple(params_dict.keys()))\n",
    "params_tuple = deepcopy(tuple(params_dict.values()))\n",
    "\n",
    "# make model functional using functorch\n",
    "func_model, params = ft.make_functional(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Use `functional_call` to make model functional.\"\"\"\n",
    "    preds = model(inputs)\n",
    "    return torch.mean((preds - targets) ** 2)\n",
    "\n",
    "\n",
    "def functional_call_loss(\n",
    "    param_dict: Dict[str, torch.Tensor], inputs: torch.Tensor, targets: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Use `functional_call` to make model functional.\"\"\"\n",
    "    preds = functional_call(model, param_dict, inputs)\n",
    "    return torch.mean((preds - targets) ** 2)\n",
    "\n",
    "\n",
    "def functional_call_loss_tuples(\n",
    "    *args, names: List[str], inputs: torch.Tensor, targets: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Use `functional_call` but use *args to parametrize loss.\"\"\"\n",
    "    param_dict = dict(zip(names, args))\n",
    "    preds = functional_call(model, param_dict, inputs)\n",
    "    return torch.mean((preds - targets) ** 2)\n",
    "\n",
    "\n",
    "def functorch_loss(\n",
    "    params: Tuple[torch.nn.parameter.Parameter],\n",
    "    inputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Use functorch to make model functional.\"\"\"\n",
    "    preds = func_model(params, inputs)\n",
    "    return torch.mean((preds - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7264, grad_fn=<MeanBackward0>),\n",
       " tensor(0.7264, grad_fn=<MeanBackward0>),\n",
       " tensor(0.7264, grad_fn=<MeanBackward0>),\n",
       " tensor(0.7264, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    loss(inputs, targets),\n",
    "    functional_call_loss(params_dict, inputs, targets),\n",
    "    functional_call_loss_tuples(\n",
    "        *params_tuple, names=names, inputs=inputs, targets=targets\n",
    "    ),\n",
    "    functorch_loss(params, inputs=inputs, targets=targets),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.6 µs ± 552 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit loss(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 µs ± 3.56 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit functional_call_loss(params_dict, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.1 µs ± 2.66 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit functional_call_loss_tuples(*params_tuple, names=names, inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.2 µs ± 1.14 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit functorch_loss(params, inputs=inputs, targets=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Derivatives wrt parameters\n",
    "\n",
    "Compute \n",
    "$\\nabla_W L(D; \\theta)$\n",
    "and\n",
    "$\\nabla_b L(D; \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the definition of scalar-by-vector derivative look [here](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector)\n",
    "\n",
    "\n",
    "Analytical solutions\n",
    "\n",
    "$$\\frac{d L}{d W} = \\left[ \\frac{d L}{d w_1}, \\; \\frac{d L}{d w_2}\\right] = \\left[2 x_1(Wx + b - y) , \\; 2 x_2 (Wx + b - y)\\right]$$\n",
    "\n",
    "$$\\frac{d L}{d b} = 2 (Wx + b - y) (+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4424, -0.5870]), tensor([-1.7045]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dw = 2 * (model(inputs) - targets) * inputs\n",
    "    db = 2 * (model(inputs) - targets)\n",
    "\n",
    "dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 µs ± 14.9 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4424, -0.5870]]), tensor([-1.7045]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit grad(loss(inputs, targets), model.parameters())\n",
    "\n",
    "grad(loss(inputs, targets), model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 µs ± 7.03 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4424, -0.5870]]), tensor([-1.7045]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit grad(functional_call_loss(params_dict, inputs, targets), params_dict.values())\n",
    "\n",
    "grad(functional_call_loss(params_dict, inputs, targets), params_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245 µs ± 1.97 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4424, -0.5870]]), tensor([-1.7045]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit grad(functional_call_loss_tuples(*params_tuple, names=names, inputs=inputs, targets=targets), params_tuple)\n",
    "\n",
    "grad(\n",
    "    functional_call_loss_tuples(\n",
    "        *params_tuple, names=names, inputs=inputs, targets=targets\n",
    "    ),\n",
    "    params_tuple,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 µs ± 27.1 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4424, -0.5870]], grad_fn=<TBackward0>),\n",
       " tensor([-1.7045], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit ft.grad(functorch_loss)(params, inputs, targets)\n",
    "\n",
    "ft.grad(functorch_loss)(params, inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Second-order derivatives wrt parameters\n",
    "\n",
    "Compute $\\nabla_W^2 L(D; \\theta)$ and $\\nabla_b^2 L(D; \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the definition of scalar-by-vector derivative look [here](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector)\n",
    "\n",
    "\n",
    "Analytical solutions\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial^2 w_1} = 2x_1^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial^2 w_2} = 2x_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial^2 b} = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial w_1 \\; \\partial w_2} = \\frac{\\partial^2 L}{\\partial w_2 \\; \\partial w_1} 2x_1 x_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial w_1 \\; \\partial b} = \\frac{\\partial^2 L}{\\partial b \\; \\partial w_1} = 2x_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial w_2 \\; \\partial b} = \\frac{\\partial^2 L}{\\partial b \\; \\partial w_2} = 2x_2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1347, 0.2372]) tensor(2) tensor(0.1788) tensor([0.5191, 0.6888])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\n",
    "        2 * (inputs**2),\n",
    "        torch.tensor(2),\n",
    "        2 * inputs.prod(),\n",
    "        2 * inputs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hessian():\n",
    "    first_order_grads = grad(\n",
    "        loss(inputs, targets), model.parameters(), create_graph=True\n",
    "    )\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    return [\n",
    "        [grad(elem, param, create_graph=True)[0] for elem in g.flatten()]\n",
    "        for param, g in product(model.parameters(), first_order_grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975 µs ± 7.69 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[tensor([[0.1347, 0.1788]], grad_fn=<TBackward0>),\n",
       "  tensor([[0.1788, 0.2372]], grad_fn=<TBackward0>)],\n",
       " [tensor([[0.5191, 0.6888]], grad_fn=<TBackward0>)],\n",
       " [tensor([0.5191], grad_fn=<MulBackward0>),\n",
       "  tensor([0.6888], grad_fn=<MulBackward0>)],\n",
       " [tensor([2.], grad_fn=<MulBackward0>)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit my_hessian()\n",
    "\n",
    "my_hessian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hessian_functional_call():\n",
    "    first_order_grads = grad(\n",
    "        functional_call_loss(params_dict, inputs, targets),\n",
    "        params_dict.values(),\n",
    "        create_graph=True,\n",
    "    )\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    return [\n",
    "        [grad(elem, param, create_graph=True)[0] for elem in g.flatten()]\n",
    "        for param, g in product(params_dict.values(), first_order_grads)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12 ms ± 37.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[tensor([[0.1347, 0.1788]], grad_fn=<TBackward0>),\n",
       "  tensor([[0.1788, 0.2372]], grad_fn=<TBackward0>)],\n",
       " [tensor([[0.5191, 0.6888]], grad_fn=<TBackward0>)],\n",
       " [tensor([0.5191], grad_fn=<MulBackward0>),\n",
       "  tensor([0.6888], grad_fn=<MulBackward0>)],\n",
       " [tensor([2.], grad_fn=<MulBackward0>)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit my_hessian_functional_call()\n",
    "\n",
    "my_hessian_functional_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(*params):\n",
    "    return functional_call_loss_tuples(\n",
    "        *params, names=names, inputs=inputs, targets=targets\n",
    "    )\n",
    "\n",
    "\n",
    "def torch_hessian(loss, params):\n",
    "    return hessian(loss, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 µs ± 31.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[[[0.1347, 0.1788]],\n",
       "  \n",
       "           [[0.1788, 0.2372]]]]),\n",
       "  tensor([[[0.5191],\n",
       "           [0.6888]]])),\n",
       " (tensor([[[0.5191, 0.6888]]]), tensor([[2.]])))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit torch_hessian(l, params_tuple)\n",
    "\n",
    "torch_hessian(l, params_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functorch_hessian():\n",
    "    return ft.hessian(functorch_loss)(params, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.57 ms ± 47.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[[[0.1347, 0.1788]],\n",
       "  \n",
       "           [[0.1788, 0.2372]]]], grad_fn=<ViewBackward0>),\n",
       "  tensor([[[0.5191],\n",
       "           [0.6888]]], grad_fn=<ViewBackward0>)),\n",
       " (tensor([[[0.5191, 0.6888]]], grad_fn=<ViewBackward0>),\n",
       "  tensor([[2.]], grad_fn=<ViewBackward0>)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit functorch_hessian()\n",
    "\n",
    "functorch_hessian()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hessian-vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0494, -1.3924]]), tensor([-4.0431]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_order_grads = grad(\n",
    "    loss(inputs, targets), model.parameters(), create_graph=True\n",
    ")\n",
    "hvp = grad(first_order_grads, model.parameters(), grad_outputs=first_order_grads)\n",
    "hvp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check analytically if this is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0494, -1.3924, -4.0431])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1, x_2 = inputs\n",
    "\n",
    "# manually construct the Hessian\n",
    "H = torch.tensor(\n",
    "    [\n",
    "        [2 * x_1**2, 2 * x_1 * x_2, 2 * x_1],\n",
    "        [2 * x_1 * x_2, 2 * x_2**2, 2 * x_2],\n",
    "        [2 * x_1, 2 * x_2, 2],\n",
    "    ]\n",
    ")\n",
    "\n",
    "v = grad(loss(inputs, targets), model.parameters())\n",
    "v = torch.cat(tuple(i.flatten() for i in v))\n",
    "\n",
    "# multiply by gradients\n",
    "torch.matmul(H, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Check how `grad` works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n",
      "(tensor([[0.1347, 0.1788]], grad_fn=<TBackward0>),)\n",
      "(tensor([[0.1788, 0.2372]], grad_fn=<TBackward0>),)\n",
      "(tensor([2.], grad_fn=<MulBackward0>),)\n",
      "\n",
      "wrong: it sums the gradients\n",
      "(tensor([[0.3135, 0.4160]], grad_fn=<TBackward0>),)\n",
      "(tensor([2.], grad_fn=<MulBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "first_order_grads = grad(\n",
    "    functional_call_loss(params_dict, inputs, targets),\n",
    "    params_dict.values(),\n",
    "    create_graph=True,\n",
    ")\n",
    "\n",
    "print(\"correct\")\n",
    "for param, grads in zip(params_dict.values(), first_order_grads):\n",
    "    flat_grad = grads.flatten()\n",
    "    # this gives the right answers\n",
    "    for idx in range(len(flat_grad)):\n",
    "        print(grad(flat_grad[idx], param, create_graph=True))\n",
    "\n",
    "print(\"\\nwrong: it sums the gradients\")\n",
    "for param, grads in zip(params_dict.values(), first_order_grads):\n",
    "    flat_grad = grads.flatten()\n",
    "    # this does not work since it sums the gradients in the tuple\n",
    "    print(grad(tuple(flat_grad), param, create_graph=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0215, -2.4176]]), tensor([-3.9874]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.rand((1, 2), requires_grad=True)\n",
    "targets = torch.rand((1, 1), requires_grad=True)\n",
    "\n",
    "\n",
    "first_order_grads = grad(\n",
    "    loss(inputs, targets), model.parameters(), create_graph=True\n",
    ")\n",
    "hvp = grad(first_order_grads, model.parameters(), grad_outputs=first_order_grads)\n",
    "hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('influence')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e29dca69b5c2594f7e2ce2e6342c7a8fe5b5cf82dacbb30994509402d2c8e0ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

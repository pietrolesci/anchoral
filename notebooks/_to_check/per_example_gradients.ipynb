{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from lightning.fabric import seed_everything\n",
    "from torch.func import functional_call, grad, vmap\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from energizer.datastores import PandasDataStoreForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 10069,  2005,  1056,  1050, 11550,  2044,  7566,  9209,  5052,\n",
       "           3667,  2012,  6769,  2047,  8095,  2360,  2027,  2024,  1005,  9364,\n",
       "           1005,  2044,  7566,  2007, 16654,  6687,  3813,  2976,  9587, 24848,\n",
       "           1012,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([0]),\n",
       " <InputKeys.ON_CPU: 'on_cpu'>: {}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = PandasDataStoreForSequenceClassification.load(\"../data/prepared/agnews_binarised_bert-tiny/\")\n",
    "ds.show_batch(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading(batch_size=1)\n",
    "sample = next(iter(ds.train_loader(passive=True)))\n",
    "s_inp, s_att, s_lab = (\n",
    "    sample[\"input_ids\"].flatten(),\n",
    "    sample[\"attention_mask\"].flatten(),\n",
    "    sample[\"labels\"].flatten(),\n",
    ")\n",
    "\n",
    "ds.prepare_for_loading(batch_size=2)\n",
    "batch = next(iter(ds.train_loader(passive=True)))\n",
    "b_inp, b_att, b_lab = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {k: v.detach() for k, v in model.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in model.named_buffers()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8419)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(params: Dict, buffers: Dict, input_ids, attention_mask, labels) -> torch.Tensor:\n",
    "    inp, att, lab = (\n",
    "        input_ids.unsqueeze(0),\n",
    "        attention_mask.unsqueeze(0),\n",
    "        labels.unsqueeze(0),\n",
    "    )\n",
    "    return functional_call(model, (params, buffers), (inp, att), kwargs={\"labels\": lab}).loss\n",
    "\n",
    "\n",
    "compute_loss(params, buffers, s_inp, s_att, s_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8419, 0.7595])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_vect = vmap(compute_loss, in_dims=(None, None, 0, 0, 0))\n",
    "compute_loss_vect(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.894059181213379, 7.85386848449707]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_grad = grad(compute_loss)\n",
    "\n",
    "\n",
    "def grad_norm(grads, norm_type):\n",
    "    norms = [g.norm(norm_type).unsqueeze(0) for g in grads.values() if g is not None]\n",
    "    return torch.concat(norms).norm(norm_type)\n",
    "\n",
    "\n",
    "def compute_grad_norm(\n",
    "    params: Dict, buffers: Dict, input_ids, attention_mask, labels\n",
    ") -> torch.Tensor:\n",
    "    grads = compute_grad(params, buffers, input_ids, attention_mask, labels)\n",
    "    return grad_norm(grads, 2)\n",
    "\n",
    "\n",
    "compute_grad_norm_vect = vmap(compute_grad_norm, in_dims=(None, None, 0, 0, 0))\n",
    "compute_grad_norm_vect(params, buffers, b_inp, b_att, b_lab).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_grad_vect = vmap(compute_grad, in_dims=(None, None, 0, 0, 0))\n",
    "grads_vect = compute_grad_vect(params, buffers, b_inp, b_att, b_lab)\n",
    "grads_vect[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(grads.values())[0].norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.pop(\"on_cpu\", None)\n",
    "model(**sample).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = vmap(compute_loss, in_dims=(None, None, 0, 0, 0))\n",
    "batch.pop(\"on_cpu\", None)\n",
    "f(params, buffers, batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_grad = grad(compute_loss)\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_compute_sample_grad(\n",
    "    params, buffers, batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading(batch_size=2)\n",
    "batch = next(iter(ds.train_loader(passive=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.pop(\"on_cpu\", None)\n",
    "ft_compute_sample_grad(params, buffers, **batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "params = {k: v.detach() for k, v in model.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in model.named_buffers()}\n",
    "\n",
    "\n",
    "def compute_loss(params: Dict, buffers: Dict, input_ids, attention_mask, labels) -> torch.Tensor:\n",
    "    inp, att, lab = (\n",
    "        input_ids.unsqueeze(0),\n",
    "        attention_mask.unsqueeze(0),\n",
    "        labels.unsqueeze(0),\n",
    "    )\n",
    "    return functional_call(model, (params, buffers), (inp, att), kwargs={\"labels\": lab}).loss\n",
    "\n",
    "\n",
    "compute_grad = grad(compute_loss)\n",
    "\n",
    "\n",
    "def grad_norm(grads, norm_type):\n",
    "    norms = [g.norm(norm_type).unsqueeze(0) for g in grads.values() if g is not None]\n",
    "    return torch.concat(norms).norm(norm_type)\n",
    "\n",
    "\n",
    "def compute_grad_norm(\n",
    "    params: Dict, buffers: Dict, input_ids, attention_mask, labels\n",
    ") -> torch.Tensor:\n",
    "    grads = compute_grad(params, buffers, input_ids, attention_mask, labels)\n",
    "    return grad_norm(grads, 2)\n",
    "\n",
    "\n",
    "compute_grad_norm_vect = vmap(compute_grad_norm, in_dims=(None, None, 0, 0, 0), randomness=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading(batch_size=2)\n",
    "batch = next(iter(ds.train_loader(passive=True)))\n",
    "b_inp, b_att, b_lab = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.8941, 7.8539])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_grad_norm_vect(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-4.1018e-03, -3.0695e-02, -3.5295e-03,  ...,  1.8925e-02,\n",
       "          3.7396e-03, -2.9233e-03],\n",
       "        [-4.2748e-04, -3.6929e-02, -1.7168e-02,  ...,  2.9314e-02,\n",
       "         -1.0398e-02,  2.6772e-02],\n",
       "        [ 5.9418e-03,  4.2119e-03, -1.9566e-02,  ...,  1.6799e-02,\n",
       "         -2.7802e-02, -6.9017e-03],\n",
       "        ...,\n",
       "        [ 3.5573e-02, -1.5891e-02,  4.9951e-03,  ...,  5.4071e-03,\n",
       "         -1.1270e-02, -6.9528e-05],\n",
       "        [-8.7018e-03, -2.2516e-02,  3.1993e-03,  ...,  2.7591e-02,\n",
       "         -1.9554e-02,  2.4023e-03],\n",
       "        [-7.8904e-02, -7.5407e-02, -4.6660e-03,  ..., -5.3340e-03,\n",
       "         -4.4993e-02,  5.9842e-02]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric = Fabric(accelerator=\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fabric.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

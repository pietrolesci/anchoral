{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from lightning.fabric import seed_everything\n",
    "from torch import Tensor\n",
    "from torch.func import functional_call, grad, hessian, jacfwd, jacrev, jvp, vmap  # type: ignore\n",
    "from torch.types import Device\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from energizer.datastores import PandasDataStoreForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher(\n",
    "    shape,\n",
    "    generator: Optional[torch.Generator] = None,\n",
    "    device: Optional[Device] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Sample from Rademacher distribution.\"\"\"\n",
    "    return torch.randint(0, 2, shape, generator=generator, device=device) * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafad8bbb60d4b979230d4aec4dab11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-999f0e823471e99e_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-7c0ef1ff89bb0eab_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/pl487/.cache/huggingface/datasets/pietrolesci___parquet/pietrolesci--pubmed-rct20k_indexed-58c1c04dc03e65ed/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-db9896ae5bc093c4_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ds_dict = load_dataset(\"pietrolesci/pubmed-rct20k_indexed\").map(\n",
    "    lambda ex: tokenizer(ex[\"text\"]), batched=True, num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PandasDataStoreForSequenceClassification()\n",
    "ds.from_dataset_dict(\n",
    "    ds_dict,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    target_name=\"labels\",\n",
    "    tokenizer=tokenizer,\n",
    "    uid_name=\"uid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2817, 16578, 11290,  3853, 28828,  1999,  2540,  4945,\n",
       "           5022,  4914,  2007,  5729, 11325, 21933,  8737,  6132,  4383,  2540,\n",
       "           4945,  1006,  4748,  2232,  2546,  1007,  1012,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1]]),\n",
       " <InputKeys.TARGET: 'labels'>: tensor([0])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = ds.show_batch(\"test\")\n",
    "_ = batch.pop(\"on_cpu\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.label(list(range(100)), round=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.train_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading(batch_size=32, eval_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "# load model using data properties\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,  # type: ignore\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "params = {k: v.detach() for k, v in model.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in model.named_buffers()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {k: v.detach() for k, v in model.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in model.named_buffers()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute full gradient\n",
    "for batch in ds.train_loader():\n",
    "    batch.pop(\"on_cpu\")\n",
    "    loss = model(**batch).loss\n",
    "    loss.backward()\n",
    "\n",
    "full_grad = {n: p.grad for n, p in model.named_parameters() if p.requires_grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_inp, b_att, b_lab = batch.values()\n",
    "inp, att, lab = b_inp[0].squeeze(), b_att[0].squeeze(), b_lab[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "\n",
    "\n",
    "def _compute_loss(\n",
    "    model,\n",
    "    params: Dict,\n",
    "    buffers: Dict,\n",
    "    input_ids: Tensor,\n",
    "    attention_mask: Tensor,\n",
    "    labels: Tensor,\n",
    ") -> torch.Tensor:\n",
    "    inp, att, lab = (\n",
    "        input_ids.unsqueeze(0),\n",
    "        attention_mask.unsqueeze(0),\n",
    "        labels.unsqueeze(0),\n",
    "    )\n",
    "    return functional_call(\n",
    "        model, (params, buffers), (inp, att), kwargs={\"labels\": lab}\n",
    "    ).loss\n",
    "\n",
    "\n",
    "compute_loss = partial(_compute_loss, model)\n",
    "compute_loss_batch = vmap(compute_loss, in_dims=(None, None, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5007)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(params, buffers, inp, att, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5007, 1.5600, 1.5448, 1.4515])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss_batch(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads\n",
    "compute_grad = grad(compute_loss)\n",
    "compute_grad_batch = vmap(compute_grad, in_dims=(None, None, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_grad = compute_grad(params, buffers, inp, att, lab)\n",
    "len(sample_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_grad = compute_grad_batch(params, buffers, b_inp, b_att, b_lab)\n",
    "len(batch_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm\n",
    "\n",
    "\n",
    "def _gradnorm(grads: Dict, norm_type: int) -> Tensor:\n",
    "    norms = [g.norm(norm_type).unsqueeze(0) for g in grads.values() if g is not None]\n",
    "    return torch.concat(norms).norm(norm_type)\n",
    "\n",
    "\n",
    "gradnorm = partial(_gradnorm, norm_type=2)\n",
    "\n",
    "\n",
    "def compute_gradnorm(params, buffers, input_ids, attention_mask, labels):\n",
    "    grads = compute_grad(params, buffers, input_ids, attention_mask, labels)\n",
    "    return gradnorm(grads)\n",
    "\n",
    "\n",
    "compute_gradnorm_batch = vmap(\n",
    "    compute_gradnorm, in_dims=(None, None, 0, 0, 0), randomness=\"same\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8751)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradnorm(params, buffers, inp, att, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.8751, 10.0391,  8.4983, 10.0508])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradnorm_batch(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm of project along the full gradient\n",
    "\n",
    "\n",
    "def _gradnorm_full(grad: Dict, full_grad: Dict, norm_type: int) -> Tensor:\n",
    "    # compute product\n",
    "    prods = {k: full_grad[k] * grad[k] for k in full_grad}\n",
    "    return _gradnorm(prods, norm_type)\n",
    "\n",
    "\n",
    "gradnorm_full = partial(_gradnorm_full, full_grad=full_grad, norm_type=2)\n",
    "\n",
    "\n",
    "def compute_gradnorm_full(params, buffers, input_ids, attention_mask, labels):\n",
    "    grads = compute_grad(params, buffers, input_ids, attention_mask, labels)\n",
    "    return gradnorm_full(grads)\n",
    "\n",
    "\n",
    "compute_gradnorm_full_batch = vmap(\n",
    "    compute_gradnorm_full, in_dims=(None, None, 0, 0, 0), randomness=\"same\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2527)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradnorm_full(params, buffers, inp, att, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2526, 3.5917, 2.0297, 3.9550])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradnorm_full_batch(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity with full gradient\n",
    "\n",
    "\n",
    "def _cosine_similarity(grad, full_grad):\n",
    "    return torch.nn.functional.cosine_similarity(\n",
    "        torch.cat([p.flatten() for p in full_grad.values()]),\n",
    "        torch.cat([p.flatten() for p in grad.values()]),\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "\n",
    "cosine_similarity = partial(_cosine_similarity, full_grad=full_grad)\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(params, buffers, input_ids, attention_mask, labels):\n",
    "    grads = compute_grad(params, buffers, input_ids, attention_mask, labels)\n",
    "    return cosine_similarity(grads)\n",
    "\n",
    "\n",
    "compute_cosine_similarity_batch = vmap(\n",
    "    compute_cosine_similarity, in_dims=(None, None, 0, 0, 0), randomness=\"same\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6617)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity(params, buffers, inp, att, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6617,  0.4107, -0.0971,  0.6107])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine_similarity_batch(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance from full gradient normed\n",
    "\n",
    "\n",
    "def _diff(grad, full_grad):\n",
    "    return torch.cat([p.flatten() for p in grad.values()]) - torch.cat(\n",
    "        [p.flatten() for p in full_grad.values()]\n",
    "    )\n",
    "\n",
    "\n",
    "diff = partial(_diff, full_grad=full_grad)\n",
    "\n",
    "\n",
    "def compute_diff(params, buffers, input_ids, attention_mask, labels):\n",
    "    grads = compute_grad(params, buffers, input_ids, attention_mask, labels)\n",
    "    return diff(grads).norm(2)\n",
    "\n",
    "\n",
    "compute_diff_batch = vmap(\n",
    "    compute_diff, in_dims=(None, None, 0, 0, 0), randomness=\"same\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.9565)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff(params, buffers, inp, att, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.9565, 10.5898, 13.3072,  8.6203])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_diff_batch(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(f, primals, tangents):\n",
    "    return jvp(grad(f), primals, tangents)[1]\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x.sin().sum()\n",
    "\n",
    "\n",
    "x = torch.randn(2048)\n",
    "tangent = torch.randn(2048)\n",
    "\n",
    "result = hvp(f, (x,), (tangent,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#compute_grad(inp, att, lab), \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vmap(grad(compute_loss))(b_inp, b_att, b_lab)\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/vmap.py:434\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    431\u001b[0m                          args_spec, out_dims, randomness, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    433\u001b[0m \u001b[39m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    435\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    436\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/vmap.py:619\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 619\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mbatched_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    621\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1380\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m   1379\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1380\u001b[0m     results \u001b[39m=\u001b[39m grad_and_value(func, argnums, has_aux\u001b[39m=\u001b[39;49mhas_aux)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1381\u001b[0m     \u001b[39mif\u001b[39;00m has_aux:\n\u001b[1;32m   1382\u001b[0m         grad, (_, aux) \u001b[39m=\u001b[39m results\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/vmap.py:39\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     38\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 39\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1243\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m kwargs \u001b[39m=\u001b[39m _wrap_all_tensors(kwargs, level)\n\u001b[1;32m   1242\u001b[0m diff_args \u001b[39m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1243\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[39m=\u001b[39;49mlevel), diff_args)\n\u001b[1;32m   1245\u001b[0m output \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1246\u001b[0m \u001b[39mif\u001b[39;00m has_aux:\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/pytree_hacks.py:12\u001b[0m, in \u001b[0;36mtree_map_\u001b[0;34m(fn_, pytree)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map_\u001b[39m(fn_, pytree):\n\u001b[1;32m     11\u001b[0m     flat_args, _ \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m---> 12\u001b[0m     [fn_(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m flat_args]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m pytree\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/pytree_hacks.py:12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map_\u001b[39m(fn_, pytree):\n\u001b[1;32m     11\u001b[0m     flat_args, _ \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m---> 12\u001b[0m     [fn_(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m flat_args]\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m pytree\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:57\u001b[0m, in \u001b[0;36m_create_differentiable\u001b[0;34m(inps, level)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mrequires_grad_()\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThing passed to transform API must be Tensor, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[39mreturn\u001b[39;00m tree_map(create_differentiable, inps)\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[39m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/.conda/envs/igal/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:54\u001b[0m, in \u001b[0;36m_create_differentiable.<locals>.create_differentiable\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     53\u001b[0m     \u001b[39mwith\u001b[39;00m enable_inplace_requires_grad():\n\u001b[0;32m---> 54\u001b[0m         \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mrequires_grad_()\n\u001b[1;32m     55\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThing passed to transform API must be Tensor, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "# compute_grad(inp, att, lab),\n",
    "vmap(grad(compute_loss))(b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_grad_norm_vect(b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = vmap(compute_grad, in_dims=(None, None, 0, 0, 0), randomness=\"same\")(\n",
    "    params, buffers, b_inp, b_att, b_lab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap(jacrev(f), in_dims=(None, None, 0, 0, 0), randomness=\"same\")(params, buffers, b_inp, b_att, b_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = [rademacher(p.size()) for p in grads.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvp(compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(self, maxIter=100, tol=1e-3):\n",
    "    \"\"\"\n",
    "    compute the trace of hessian using Hutchinson's method\n",
    "    maxIter: maximum iterations used to compute trace\n",
    "    tol: the relative tolerance\n",
    "    \"\"\"\n",
    "\n",
    "    device = self.device\n",
    "    trace_vhv = []\n",
    "    trace = 0.0\n",
    "\n",
    "    for i in range(maxIter):\n",
    "        self.model.zero_grad()\n",
    "        v = [torch.randint_like(p, high=2, device=device) for p in self.params]\n",
    "        # generate Rademacher random variables\n",
    "        for v_i in v:\n",
    "            v_i[v_i == 0] = -1\n",
    "\n",
    "        if self.full_dataset:\n",
    "            _, Hv = self.dataloader_hv_product(v)\n",
    "        else:\n",
    "            Hv = hessian_vector_product(self.gradsH, self.params, v)\n",
    "        trace_vhv.append(group_product(Hv, v).cpu().item())\n",
    "        if abs(np.mean(trace_vhv) - trace) / (abs(trace) + 1e-6) < tol:\n",
    "            return trace_vhv\n",
    "        else:\n",
    "            trace = np.mean(trace_vhv)\n",
    "\n",
    "    return trace_vhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms, ids = [], []\n",
    "for batch in tqdm(datastore.train_loader(), disable=True):\n",
    "    batch = self.transfer_to_device(batch)\n",
    "    b_inp, b_att, b_lab = (\n",
    "        batch[InputKeys.INPUT_IDS],\n",
    "        batch[InputKeys.ATT_MASK],\n",
    "        batch[InputKeys.TARGET],\n",
    "    )\n",
    "    norms += compute_grad_norm_vect(params, buffers, b_inp, b_att, b_lab).tolist()\n",
    "    ids += batch[InputKeys.ON_CPU][SpecialKeys.ID]\n",
    "\n",
    "norms = np.array(norms)\n",
    "ids = np.array(ids)\n",
    "topk_ids = norms.argsort()[-self.num_influential :]  # biggest gradient norm\n",
    "return ids[topk_ids].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "loss = model(**batch).loss\n",
    "torch.autograd.backward(loss, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "generator = torch.Generator().manual_seed(2147483647)\n",
    "grads = [p.grad for p in params]\n",
    "n_samples = 10\n",
    "\n",
    "\n",
    "for i in range(n_samples):\n",
    "    zs = [\n",
    "        torch.randint(0, 2, p.size(), generator=generator) * 2.0 - 1.0\n",
    "        for p in params\n",
    "    ]  # Rademacher distribution {-1.0, 1.0}\n",
    "\n",
    "    h_zs = torch.autograd.grad(grads, params, grad_outputs=zs, retain_graph=False)\n",
    "\n",
    "    if i == 0:\n",
    "        hess = h_zs\n",
    "    else:\n",
    "        for d, hvp, v in zip(hess, h_zs, zs):\n",
    "            d += v * hvp / n_samples\n",
    "\n",
    "    # for idx, (h_z, z) in enumerate(zip(h_zs, zs)):\n",
    "    #     hess[idx] += h_z / n_samples  # approximate the expected values of z*(H@z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "generator = torch.Generator().manual_seed(2147483647)\n",
    "grads = [p.grad for p in params]\n",
    "n_samples = 10\n",
    "\n",
    "for _ in range(V):\n",
    "    for p in model.parameters():\n",
    "        v = [rademacher(p.size())]\n",
    "        Hv = hessian_vector_product(loss, [p], v)\n",
    "        vHv = torch.einsum(\"i,i->\", v[0].flatten(), Hv[0].flatten())\n",
    "\n",
    "        trace += vHv / V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

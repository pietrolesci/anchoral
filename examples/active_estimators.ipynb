{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energizer.datastores import PandasDataStoreForSequenceClassification\n",
    "from energizer.estimators.estimator import Estimator\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import Accuracy, F1Score, Precision, Recall\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from energizer.enums import InputKeys, OutputKeys, RunningStage\n",
    "import numpy as np\n",
    "from energizer.utilities import move_to_cpu\n",
    "from lightning.fabric.loggers import TensorBoardLogger\n",
    "from lightning.fabric import seed_everything\n",
    "from energizer.callbacks import GradNorm, PytorchTensorboardProfiler, EarlyStopping, ModelCheckpoint\n",
    "from energizer.strategies import RandomStrategy, UncertaintyBasedStrategy\n",
    "from energizer.strategies.random import RandomStrategySEALS\n",
    "from energizer.strategies.uncertainty import UncertaintyBasedStrategySEALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorForSequenceClassification(Estimator):\n",
    "\n",
    "    def train_step(self, model, batch, batch_idx, loss_fn, metrics: MetricCollection) -> Dict:\n",
    "        return self.step(model, batch, metrics, RunningStage.TRAIN)\n",
    "\n",
    "    def validation_step(self, model, batch, batch_idx, loss_fn, metrics: MetricCollection) -> Dict:\n",
    "        return self.step(model, batch, metrics, RunningStage.VALIDATION)\n",
    "\n",
    "    def test_step(self, model, batch, batch_idx, loss_fn, metrics: MetricCollection) -> Dict:\n",
    "        return self.step(model, batch, metrics, RunningStage.TEST)\n",
    "    \n",
    "    def train_epoch_end(self, output: List[np.ndarray], metrics: MetricCollection) -> Dict:\n",
    "        return self.epoch_end(output, metrics, RunningStage.TRAIN)\n",
    "\n",
    "    def validation_epoch_end(self, output: List[np.ndarray], metrics: MetricCollection) -> Dict:\n",
    "        return self.epoch_end(output, metrics, RunningStage.VALIDATION)\n",
    "\n",
    "    def test_epoch_end(self, output: List[np.ndarray], metrics: MetricCollection) -> Dict:\n",
    "        return self.epoch_end(output, metrics, RunningStage.TEST)\n",
    "\n",
    "    def step(self, model, batch: Dict, metrics: MetricCollection, stage: RunningStage) -> torch.Tensor:\n",
    "        _ = batch.pop(InputKeys.ON_CPU, None)\n",
    "\n",
    "        out = model(**batch)\n",
    "        out_metrics = metrics(out.logits, batch[InputKeys.TARGET])\n",
    "\n",
    "        if stage == RunningStage.TRAIN:\n",
    "            logs = {OutputKeys.LOSS: out.loss, **out_metrics}\n",
    "            self.log_dict({f\"{stage}/{k}\": v for k, v in logs.items()}, step=self.progress_tracker.global_batch)\n",
    "\n",
    "        return out.loss\n",
    "    \n",
    "    def epoch_end(self, output: List[np.ndarray], metrics: MetricCollection, stage: RunningStage) -> float:\n",
    "        aggregated_metrics = move_to_cpu(metrics.compute())  # NOTE: metrics are still on device\n",
    "        aggregated_loss = round(np.mean(output).item(), 6)\n",
    "        \n",
    "        logs = {OutputKeys.LOSS: aggregated_loss, **aggregated_metrics}\n",
    "        self.log_dict({f\"{stage}_end/{k}\": v for k, v in logs.items()}, step=self.progress_tracker.safe_global_epoch)\n",
    "\n",
    "        return aggregated_metrics\n",
    "\n",
    "    def configure_metrics(self, *_) -> MetricCollection:\n",
    "        num_classes = self.model.num_labels\n",
    "        task = \"multiclass\"\n",
    "        # NOTE: you are in charge of moving it to the correct device\n",
    "        return MetricCollection(\n",
    "            {\n",
    "                \"accuracy\": Accuracy(task, num_classes=num_classes),\n",
    "                \"f1_macro\": F1Score(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"precision_macro\": Precision(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"recall_macro\": Recall(task, num_classes=num_classes, average=\"macro\"),\n",
    "                \"f1_micro\": F1Score(task, num_classes=num_classes, average=\"micro\"),\n",
    "                \"precision_micro\": Precision(task, num_classes=num_classes, average=\"micro\"),\n",
    "                \"recall_micro\": Recall(task, num_classes=num_classes, average=\"micro\"),\n",
    "            }\n",
    "        ).to(self.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStrategySEALSForSequenceClassification(EstimatorForSequenceClassification, RandomStrategySEALS):\n",
    "    ...\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "estimator = RandomStrategySEALSForSequenceClassification(\n",
    "    model, \n",
    "    accelerator=\"gpu\", \n",
    "    loggers=[TensorBoardLogger(\"./\", name=\"tb_logs\")],\n",
    "    callbacks=[\n",
    "        GradNorm(2), \n",
    "        ModelCheckpoint(\"./checkpoints\", monitor=\"f1_macro\", stage=\"train\", mode=\"max\"),\n",
    "        EarlyStopping(monitor=\"f1_macro\", stage=\"train\", interval=\"epoch\", mode=\"max\"),\n",
    "    ],\n",
    "    seed=42,\n",
    "    num_neighbours=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading()\n",
    "results = estimator.active_fit(\n",
    "    datastore=ds, \n",
    "    query_size=50,\n",
    "    max_rounds=20, \n",
    "    min_steps=50,\n",
    "    reinit_model=True,\n",
    "    # limit_pool_batches=10, \n",
    "    # limit_test_batches=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomStrategyForSequenceClassification(EstimatorForSequenceClassification, RandomStrategy):\n",
    "    ...\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "estimator = RandomStrategyForSequenceClassification(\n",
    "    model, \n",
    "    accelerator=\"gpu\", \n",
    "    loggers=[TensorBoardLogger(\"./\", name=\"tb_logs\")],\n",
    "    callbacks=[\n",
    "        GradNorm(2), \n",
    "        ModelCheckpoint(\"./checkpoints\", monitor=\"f1_macro\", stage=\"train\", mode=\"max\"),\n",
    "        EarlyStopping(monitor=\"f1_macro\", stage=\"train\", interval=\"epoch\", mode=\"max\"),\n",
    "    ],\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading()\n",
    "results = estimator.active_fit(\n",
    "    datastore=ds, \n",
    "    query_size=50,\n",
    "    max_rounds=20, \n",
    "    min_steps=50,\n",
    "    reinit_model=True,\n",
    "    # limit_pool_batches=10, \n",
    "    # limit_test_batches=10,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Entropy SEALS strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PandasDataStoreForSequenceClassification.load(\"../data/prepared/agnews_binarised_bert-tiny/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ds.sample_from_pool(size=100, mode=\"stratified\", random_state=42)\n",
    "ds.label(ids, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  4108,  2373,  5222,  6996,  2006,  2470,  2817,  4108,  2373,\n",
       "           1998, 10891,  3751,  2097,  2025,  2031,  2000,  3477,  2005,  2047,\n",
       "           2470,  2046,  2943,  1011,  8122,  3454,  1010,  1996,  2110,  2270,\n",
       "           2326,  3222,  5451,  9857,  1012,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([0]),\n",
       " <InputKeys.ON_CPU: 'on_cpu'>: {<SpecialKeys.ID: 'unique_id'>: [2081]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.show_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimators import UncertaintyBasedStrategyGradSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655086009dec426c9f988c8c2a606aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed rounds:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b825c88ff67473faa015e1b6c05abb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Completed epochs: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c49ac539e344527808fb58ac5b99116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b10eb1a8d9d48f09f3377a97af0c2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89283016243435288c7312f3df84fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pool: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e8a791e3c443bdb673382b4e2f71ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b7529ea87b48d49882e1c25ad9b3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b2aca07613432e98a33b15cdc8b532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb2d411210b4b6da62862db28c0a475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5006c8c24a7246ec98b3ad17a404a571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1061b596f68f4b4ab061cc2d84c76253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100fb80fab654e7a8df165d49547af6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ff8eb6f01a4dcbbc562505cea65da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323231beac3d442b98a80e6425574853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01335968e9f042d9a6365900ca2b4f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac971568017f4aa6892a04bb28020251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e788c4359c024dc4be00261253ced56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8947c25d29b48aa8d3ad5b4ed41163e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d41b14eeb943d2b6476976f790609c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34013f5eccc244228109a856de451609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8092d6b3534b3a975fac8b84b19f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff0aa3bdf1d47b8b661ca1b06bf0342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fac1063549f460181f724d9c58472bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be3a941683f4ea294610ca8d47c2a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb33b8378d674e70a88b95057ad3e2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "<OutputKeys.METRICS: 'metrics'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m\n\u001b[1;32m     13\u001b[0m estimator \u001b[39m=\u001b[39m MyEstimator(\n\u001b[1;32m     14\u001b[0m     score_fn\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     model\u001b[39m=\u001b[39mmodel, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     num_influential\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m ds\u001b[39m.\u001b[39mprepare_for_loading()\n\u001b[0;32m---> 28\u001b[0m results \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mactive_fit(\n\u001b[1;32m     29\u001b[0m     datastore\u001b[39m=\u001b[39;49mds, \n\u001b[1;32m     30\u001b[0m     query_size\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m     max_rounds\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, \n\u001b[1;32m     32\u001b[0m     min_steps\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     limit_test_batches\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     34\u001b[0m     limit_pool_batches\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/estimators/active_estimator.py:71\u001b[0m, in \u001b[0;36mActiveEstimator.active_fit\u001b[0;34m(self, datastore, query_size, validation_perc, max_rounds, max_budget, validation_sampling, reinit_model, max_epochs, min_steps, learning_rate, optimizer, optimizer_kwargs, scheduler, scheduler_kwargs, model_cache_dir, log_interval, enable_progress_bar, limit_train_batches, limit_validation_batches, limit_test_batches, limit_pool_batches, num_validation_per_epoch)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m# configure progress tracking\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_tracker\u001b[39m.\u001b[39msetup(\n\u001b[1;32m     60\u001b[0m     max_rounds\u001b[39m=\u001b[39mmax_rounds,\n\u001b[1;32m     61\u001b[0m     max_budget\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mmin\u001b[39m(datastore\u001b[39m.\u001b[39mpool_size(), max_budget \u001b[39mor\u001b[39;00m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInf\u001b[39m\u001b[39m\"\u001b[39m))),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     enable_progress_bar\u001b[39m=\u001b[39menable_progress_bar,\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 71\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_active_fit(\n\u001b[1;32m     72\u001b[0m     datastore\u001b[39m=\u001b[39;49mdatastore,\n\u001b[1;32m     73\u001b[0m     replay\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     74\u001b[0m     reinit_model\u001b[39m=\u001b[39;49mreinit_model,\n\u001b[1;32m     75\u001b[0m     model_cache_dir\u001b[39m=\u001b[39;49mmodel_cache_dir,\n\u001b[1;32m     76\u001b[0m     max_epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[1;32m     77\u001b[0m     min_steps\u001b[39m=\u001b[39;49mmin_steps,\n\u001b[1;32m     78\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     79\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     80\u001b[0m     optimizer_kwargs\u001b[39m=\u001b[39;49moptimizer_kwargs,\n\u001b[1;32m     81\u001b[0m     scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     82\u001b[0m     scheduler_kwargs\u001b[39m=\u001b[39;49mscheduler_kwargs,\n\u001b[1;32m     83\u001b[0m     query_size\u001b[39m=\u001b[39;49mquery_size,\n\u001b[1;32m     84\u001b[0m     validation_sampling\u001b[39m=\u001b[39;49mvalidation_sampling,\n\u001b[1;32m     85\u001b[0m     validation_perc\u001b[39m=\u001b[39;49mvalidation_perc,\n\u001b[1;32m     86\u001b[0m     limit_train_batches\u001b[39m=\u001b[39;49mlimit_train_batches,\n\u001b[1;32m     87\u001b[0m     limit_validation_batches\u001b[39m=\u001b[39;49mlimit_validation_batches,\n\u001b[1;32m     88\u001b[0m     limit_test_batches\u001b[39m=\u001b[39;49mlimit_test_batches,\n\u001b[1;32m     89\u001b[0m     limit_pool_batches\u001b[39m=\u001b[39;49mlimit_pool_batches,\n\u001b[1;32m     90\u001b[0m     num_validation_per_epoch\u001b[39m=\u001b[39;49mnum_validation_per_epoch,\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/anchoral/energizer/energizer/estimators/active_estimator.py:136\u001b[0m, in \u001b[0;36mActiveEstimator.run_active_fit\u001b[0;34m(self, datastore, replay, reinit_model, model_cache_dir, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m         total_budget \u001b[39m=\u001b[39m datastore\u001b[39m.\u001b[39mlabelled_size(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_tracker\u001b[39m.\u001b[39mglobal_round)\n\u001b[1;32m    132\u001b[0m         \u001b[39massert\u001b[39;00m (\n\u001b[1;32m    133\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_tracker\u001b[39m.\u001b[39mbudget_tracker\u001b[39m.\u001b[39mcurrent \u001b[39m==\u001b[39m total_budget\n\u001b[1;32m    134\u001b[0m         ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogress_tracker\u001b[39m.\u001b[39mbudget_tracker\u001b[39m.\u001b[39mcurrent\u001b[39m}\u001b[39;00m\u001b[39m == \u001b[39m\u001b[39m{\u001b[39;00mtotal_budget\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 136\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactive_fit_end(output)\n\u001b[1;32m    138\u001b[0m \u001b[39m# call hook\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfabric\u001b[39m.\u001b[39mcall(\u001b[39m\"\u001b[39m\u001b[39mon_active_fit_end\u001b[39m\u001b[39m\"\u001b[39m, estimator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, datastore\u001b[39m=\u001b[39mdatastore, output\u001b[39m=\u001b[39moutput)\n",
      "File \u001b[0;32m~/anchoral/src/estimators.py:112\u001b[0m, in \u001b[0;36mSequenceClassificationMixin.active_fit_end\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mactive_fit_end\u001b[39m(\u001b[39mself\u001b[39m, output: List[ROUND_OUTPUT]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[1;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Log metrics at the end of training.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     logs \u001b[39m=\u001b[39m ld_to_dl([out[RunningStage\u001b[39m.\u001b[39mTEST][OutputKeys\u001b[39m.\u001b[39mMETRICS] \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m output])\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    114\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhparams/test_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: v[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m logs\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m    115\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhparams/test_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m_auc\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mtrapz(v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m logs\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m    116\u001b[0m     }\n",
      "File \u001b[0;32m~/anchoral/src/estimators.py:112\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mactive_fit_end\u001b[39m(\u001b[39mself\u001b[39m, output: List[ROUND_OUTPUT]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict:\n\u001b[1;32m    111\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Log metrics at the end of training.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     logs \u001b[39m=\u001b[39m ld_to_dl([out[RunningStage\u001b[39m.\u001b[39;49mTEST][OutputKeys\u001b[39m.\u001b[39;49mMETRICS] \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m output])\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    114\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhparams/test_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m: v[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem() \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m logs\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m    115\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhparams/test_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m_auc\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mtrapz(v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m logs\u001b[39m.\u001b[39mitems()},\n\u001b[1;32m    116\u001b[0m     }\n",
      "\u001b[0;31mKeyError\u001b[0m: <OutputKeys.METRICS: 'metrics'>"
     ]
    }
   ],
   "source": [
    "\n",
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "class MyEstimator(EstimatorForSequenceClassification, UncertaintyBasedStrategyGradSub):\n",
    "    ...\n",
    "\n",
    "\n",
    "estimator = MyEstimator(\n",
    "    score_fn=\"entropy\",\n",
    "    model=model, \n",
    "    accelerator=\"gpu\", \n",
    "    loggers=[TensorBoardLogger(\"./\", name=\"tb_logs\")],\n",
    "    callbacks=[\n",
    "        GradNorm(2), \n",
    "        ModelCheckpoint(\"./checkpoints\", monitor=\"f1_macro\", stage=\"train\", mode=\"max\"),\n",
    "        EarlyStopping(monitor=\"f1_macro\", stage=\"train\", interval=\"epoch\", mode=\"max\"),\n",
    "    ],\n",
    "    num_neighbours=100,\n",
    "    num_influential=10,\n",
    ")\n",
    "\n",
    "ds.prepare_for_loading()\n",
    "results = estimator.active_fit(\n",
    "    datastore=ds, \n",
    "    query_size=50,\n",
    "    max_rounds=20, \n",
    "    min_steps=50,\n",
    "    limit_test_batches=2,\n",
    "    limit_pool_batches=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = ds.show_batch()\n",
    "batch.pop(\"on_cpu\", None)\n",
    "batch = estimator.transfer_to_device(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model1.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(estimator.model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._C._functorch.get_unwrapped(params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = estimator.fabric.setup(estimator.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " (**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyBasedStrategySEALSForSequenceClassification(EstimatorForSequenceClassification, UncertaintyBasedStrategySEALS):\n",
    "    def pool_step( self, model, batch: Dict, batch_idx: int, metrics: MetricCollection) -> Dict:\n",
    "        _ = batch.pop(InputKeys.ON_CPU)  # this is already handled in the `evaluation_step`\n",
    "        logits = model(**batch).logits\n",
    "        return self.score_fn(logits)\n",
    "\n",
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "estimator = UncertaintyBasedStrategySEALSForSequenceClassification(\n",
    "    score_fn=\"entropy\",\n",
    "    model=model, \n",
    "    accelerator=\"gpu\", \n",
    "    loggers=[TensorBoardLogger(\"./\", name=\"tb_logs\")],\n",
    "    callbacks=[\n",
    "        GradNorm(2), \n",
    "        ModelCheckpoint(\"./checkpoints\", monitor=\"f1_macro\", stage=\"train\", mode=\"max\"),\n",
    "        EarlyStopping(monitor=\"f1_macro\", stage=\"train\", interval=\"epoch\", mode=\"max\"),\n",
    "    ],\n",
    "    num_neighbours=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading()\n",
    "results = estimator.active_fit(\n",
    "    datastore=ds, \n",
    "    query_size=50,\n",
    "    max_rounds=20, \n",
    "    min_steps=50,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyBasedStrategyForSequenceClassification(EstimatorForSequenceClassification, UncertaintyBasedStrategy):\n",
    "    def pool_step( self, model, batch: Dict, batch_idx: int, metrics: MetricCollection) -> Dict:\n",
    "        _ = batch.pop(InputKeys.ON_CPU)  # this is already handled in the `evaluation_step`\n",
    "        logits = model(**batch).logits\n",
    "        return self.score_fn(logits)\n",
    "\n",
    "seed_everything(42)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    ds.tokenizer.name_or_path,\n",
    "    id2label=ds.id2label,\n",
    "    label2id=ds.label2id,\n",
    "    num_labels=len(ds.labels),\n",
    ")\n",
    "\n",
    "estimator = UncertaintyBasedStrategyForSequenceClassification(\n",
    "    score_fn=\"entropy\",\n",
    "    model=model, \n",
    "    accelerator=\"gpu\", \n",
    "    loggers=[TensorBoardLogger(\"./\", name=\"tb_logs\")],\n",
    "    callbacks=[\n",
    "        GradNorm(2), \n",
    "        ModelCheckpoint(\"./checkpoints\", monitor=\"f1_macro\", stage=\"train\", mode=\"max\"),\n",
    "        EarlyStopping(monitor=\"f1_macro\", stage=\"train\", interval=\"epoch\", mode=\"max\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.prepare_for_loading()\n",
    "results = estimator.active_fit(\n",
    "    datastore=ds, \n",
    "    query_size=50,\n",
    "    max_rounds=20, \n",
    "    min_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

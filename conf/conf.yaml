defaults:
  - _self_
  # - callbacks:
  #   # - pytorch_profiler
  #   - save_outputs
  #   - timer
  #   - model_checkpoint
  #   - early_stopping
  - loggers:
    - tensorboard
  - optional strategy: null
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

hydra:
  job:
    chdir: true
  run:
    dir: ./outputs/${experiment_group}/${dataset_name}_${now:%Y-%m-%d}T${now:%H-%M-%S}
  job_logging:
    formatters:
      colorlog:
        format: '[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s'
        log_colors:
          DEBUG: purple
          INFO: green
          WARNING: yellow
          ERROR: red
          CRITICAL: bold_red
  sweep:
    dir: ./outputs/multirun/${experiment_group}/${dataset_name}_${now:%Y-%m-%d}T${now:%H-%M-%S}
    subdir: ${hydra.job.override_dirname}

strategy: null

estimator:
  accelerator: gpu
  precision: 32
  deterministic: true

callbacks: null
loggers: null

model:
  name_or_path: null
  seed: ${seed}

data:
  batch_size: 64
  eval_batch_size: 256
  num_workers: 32
  pin_memory: true
  drop_last: false
  persistent_workers: true
  shuffle: true
  seed: ${seed}
  replacement: false
  max_source_length: 512

# this is incremental to `data` above
active_data:
  budget: null
  validation_perc: ${active_fit.validation_perc}
  sampling: ${active_fit.validation_sampling}
  seed: ${seed}

fit:
  max_epochs: 3
  min_steps: null
  learning_rate: 0.0002
  optimizer: adamw
  optimizer_kwargs: null
  scheduler: constant_schedule
  scheduler_kwargs: null
  log_interval: ${log_interval}
  enable_progress_bar: ${enable_progress_bar}
  limit_train_batches: ${limit_batches}
  limit_validation_batches: ${limit_batches}
  validation_interval: 1

# this is incremental to `fit` above
active_fit:
  max_rounds: 50
  max_budget: null
  query_size: 25
  validation_perc: 0.3
  validation_sampling: stratified
  reinit_model: true
  limit_pool_batches: ${limit_batches}
  limit_test_batches: ${limit_batches}

test:
  log_interval: ${log_interval}
  enable_progress_bar: ${enable_progress_bar}
  limit_batches: ${limit_batches}

# global settings
log_interval: 1
enable_progress_bar: true
limit_batches: null
train_val_split: null

dataset_name: '???'
seed: 42
experiment_group: dry_run
replay_path: null